# Markov Chains


## 1. What is a Markov Chain?

A **Markov Chain** is a sequence of random variables where the probability of each event depends only on the state attained in the previous event. This property is known as the **Markov Assumption**.

Mathematically, if we have a sequence of random variables $X_1, X_2, \dots, X_n$, the probability of the next variable depends only on the current state:

$$
P(X_{t+1} | X_t, X_{t-1}, \dots, X_1) = P(X_{t+1} | X_t)
$$

This assumption allows us to simplify the probability of the entire sequence as:

$$
P(X_1, X_2, \dots, X_n) = P(X_1) \cdot P(X_2 | X_1) \cdot P(X_3 | X_2) \dots P(X_n | X_{n-1})
$$

If the transition probabilities do not change over time, the process is called a **stationary Markov Chain**. In such a chain, the probability of transitioning from one state to another remains constant regardless of time.

## 2. Examples of Markov Chains

### 2.1 Discrete Markov Chain

Consider a sequence of integers between 1 and 5, where each step depends on a coin flip:
1. Flip a coin.
2. If heads, increase the number by 1 (if it’s 5, wrap back to 1).
3. If tails, decrease the number by 1 (if it’s 1, wrap back to 5).
4. Repeat for $n$ steps.

The current state only depends on the previous state, making this a discrete Markov Chain. The numbers (1 to 5) are called the **states** of the chain.

This type of chain can represent various real-life processes, such as simple games, the random movement of particles, or changes in stock prices that depend only on their immediate past values.

### 2.2 Continuous Random Walk

For a continuous example, consider a **random walk** where each step follows a normal distribution centered around the previous state:

$$
X_{t+1} \sim \mathcal{N}(X_t, 1)
$$

This is also a Markov Chain since the next state depends only on the current state. Random walks are often used in finance to model stock prices or other metrics that move incrementally with time.

## 3. Transition Matrix

A **transition matrix** lists the probabilities of moving from one state to another in a discrete Markov Chain. For a Markov Chain with 5 states, the matrix $Q$ might look like this:

$$
Q = \begin{bmatrix}
0 & 0.5 & 0 & 0 & 0.5 \\
0.5 & 0 & 0.5 & 0 & 0 \\
0 & 0.5 & 0 & 0.5 & 0 \\
0 & 0 & 0.5 & 0 & 0.5 \\
0.5 & 0 & 0 & 0.5 & 0 \\
\end{bmatrix}
$$

The entry $Q_{ij}$ represents the probability of transitioning from state $i$ to state $j$. The rows sum to 1, as each row represents a complete set of probabilities for transitioning out of a given state.

### Example Calculation

If your initial state is 1, and you want the probability of reaching state 3 after two steps, you can compute $Q^2$, the square of the transition matrix:

$$
Q^2 = Q \cdot Q
$$

#### Python Code for Matrix Multiplication

Here’s how to compute  $Q^2$ using Python:

```python
import numpy as np

# Define the transition matrix
Q = np.array([
    [0, 0.5, 0, 0, 0.5],
    [0.5, 0, 0.5, 0, 0],
    [0, 0.5, 0, 0.5, 0],
    [0, 0, 0.5, 0, 0.5],
    [0.5, 0, 0, 0.5, 0]
])

# Compute Q^2 by matrix multiplication
Q2 = np.linalg.matrix_power(Q, 2)
print("Q^2:\n", Q2)

# Probability of transitioning from state 1 to state 3 in two steps
probability = Q2[0, 2]  # indexing starts from 0 in Python
print("P(state 3 | start at state 1, after 2 steps):", probability)


```

In this case, $Q^2$ gives the probability of transitioning between states in two steps. The entry $Q^2[0,2]$ provides the probability of moving from state 1 to state 3 in two steps.

## 4. Stationary Distribution

The **stationary distribution** is the distribution that remains unchanged as the chain progresses over time. For large steps h, the probability distribution over states approaches this stationary distribution, regardless of the initial state.

### Example Calculation

For our example transition matrix Q, the stationary distribution can be approximated by raising Q to a high power, such as $Q^{30}$:

```python
# Compute Q^30 to approximate the stationary distribution
Q30 = np.linalg.matrix_power(Q, 30)
print("Q^30:\n", Q30)

```

As $Q^{30}$ (or a sufficiently high power) is computed, each row in the matrix will approach the stationary distribution, where the probabilities become uniform across all states. This means that, after many steps, the system “forgets” its starting state and each state has an equal likelihood.



## 5. Using Markov Chains for Monte Carlo Simulations

The idea here is that by simulating a Markov Chain for a long time, you can approximate a **Monte Carlo sample** from its stationary distribution. This is a fundamental idea in **Markov Chain Monte Carlo (MCMC)** methods, which are widely used in Bayesian statistics. In MCMC, instead of sampling directly from a complex posterior distribution (which is often difficult or impossible), a Markov Chain is designed such that its stationary distribution corresponds to the posterior distribution you're trying to sample from.

- **Bayesian Inference**: This technique is valuable in Bayesian inference, where the goal is to sample from a posterior distribution. By constructing a Markov Chain with the desired posterior as its stationary distribution, you can generate samples that approximate the posterior distribution.

### Continuous Random Walk with Stationary Distribution

For a continuous random walk with a modified transition:

$$
X_{t+1} \sim N(\phi X_t, 1), \quad -1 < \phi < 1
$$

This chain has a stationary distribution:

$$
N\left( 0, \frac{1}{1 - \phi^2} \right)
$$


#### Example Simulation in Python

The following code simulates this random walk and compares the results to the theoretical stationary distribution:


```python
import numpy as np
import matplotlib.pyplot as plt

# Set parameters for the simulation
np.random.seed(38)
n = 1500
phi = -0.6
x = np.zeros(n)

# Generate the Markov Chain
for i in range(1, n):
    x[i] = np.random.normal(loc=phi * x[i-1], scale=1.0)

# Plot the histogram of the chain and the theoretical stationary distribution
plt.hist(x, bins=30, density=True, alpha=0.5, label="Simulated Chain")
x_vals = np.linspace(min(x), max(x), 100)
plt.plot(x_vals, 
         (1 / np.sqrt(2 * np.pi * (1 / (1 - phi**2)))) * np.exp(-x_vals**2 / (2 * (1 / (1 - phi**2)))), 
         color='red', label="Theoretical Distribution")
plt.legend()
plt.title("Markov Chain Simulation vs. Theoretical Stationary Distribution")
plt.xlabel("State")
plt.ylabel("Density")
plt.show()

```


In this example:

- We simulate the chain for 1,500 steps.
- The histogram of the chain’s states is plotted alongside the theoretical stationary distribution $N\left( 0, \frac{1}{1 - \phi^2} \right)$.

As seen from the plot, the distribution of simulated states approximates the stationary distribution, demonstrating how a long-run Markov Chain can sample from complex distributions.

## Summary

Markov Chains provide a powerful way to model systems where the future state depends only on the current state. By understanding transition matrices and stationary distributions, we can use Markov Chains for predictive modeling and Monte Carlo simulations in applications such as Bayesian inference, finance, and natural sciences.