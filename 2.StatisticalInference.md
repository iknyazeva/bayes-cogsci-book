# Part 2: Statistical Inference

In this part, we focus on two main paradigms of inference: **Frequentist** and **Bayesian**.  
By the end of this part, you should be able to:

- Understand the likelihood function and its role in statistical inference.
- Interpret a frequentist confidence interval and know what it means (and does not mean).
- Compute a confidence interval for a Bernoulli or binomial outcome.
- Compute a maximum likelihood estimate (MLE) in simple cases.
- Derive a Bayesian posterior from a prior and a Bernoulli/binomial likelihood.
- Interpret the posterior as a representation of information and uncertainty.
- Understand the meaning and interpretation of a Bayesian posterior interval.

---

## [Frequentist inference](2.FrequentistInference)

In this section, we discuss:

- The definition and interpretation of the **likelihood function**.
- The role of **maximum likelihood estimation (MLE)**.
- How to construct and interpret **confidence intervals**, particularly for Bernoulli/binomial outcomes.
- Key contrasts between **point estimates** and **interval estimates**.

---

## [Bayesian inference](2.BayesianInference)

In this section, we cover:

- How prior information is combined with likelihoods to produce the **posterior distribution**.
- The interpretation of the posterior as both **information** and **uncertainty**.
- Posterior intervals (sometimes called **credible intervals**) and how they differ conceptually from frequentist confidence intervals.
- Worked examples with Bernoulli/binomial outcomes.

---

### [Cumulative Distribution Function](2.CDF)

In this section, we explain:

- The definition of the CDF for a random variable.
- How the CDF relates to both **probabilities** and **quantiles**.
- Its role in defining and interpreting confidence/credible intervals.
- Visualizations of CDFs for common distributions.
