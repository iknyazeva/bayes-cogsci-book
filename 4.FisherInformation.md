# Fisher Information and Its Role in Jeffreys Prior

## What is Fisher Information?

**Fisher Information** is a fundamental concept in statistics that measures the amount of information that an observable random variable $X$ carries about an unknown parameter $\theta$ of a statistical model. It provides insight into how sensitive the likelihood function is to changes in the parameter. 

Mathematically, Fisher Information $I(\theta)$ is defined as:

$$
I(\theta) = \mathbb{E} \left[ \left( \frac{\partial}{\partial \theta} \log f(X | \theta) \right)^2 \right]
$$

where $f(X | \theta)$ is the likelihood function of $X$ given the parameter $\theta$, and the expectation  $\mathbb{E}$ is taken with respect to the distribution of $X$ given $\theta$.

### Interpretation of Fisher Information

1. **Sensitivity**: A higher Fisher Information indicates that small changes in $\theta$ will cause larger changes in the likelihood function, suggesting that the data provides more information about $\theta$.
  
2. **Precision**: Fisher Information can be seen as a measure of the precision of an estimator. More information about the parameter typically results in smaller variance for the estimator.

3. **Inverse Relation**: The Fisher Information is inversely related to the variance of an estimator. In general, if the Fisher Information is high, the variance of the estimator for $\theta$ will be low, which means we can estimate $\theta$ more precisely.

### Curvature of the Likelihood Function

The **curvature of the likelihood function** is closely related to Fisher Information. Specifically, the Fisher Information can be viewed as a measure of the curvature of the log-likelihood function around the true parameter value:

- A **steep curvature** in the log-likelihood function (high Fisher Information) indicates that the likelihood changes rapidly with respect to changes in $\theta$. This means the data provides a lot of information about the parameter.
- A **flat curvature** (low Fisher Information) suggests that the likelihood function does not change much with small changes in $\theta$, indicating less information about the parameter.

The relationship can be expressed as:

$$
I(\theta) = -\mathbb{E} \left[ \frac{\partial^2}{\partial \theta^2} \log f(X | \theta) \right]
$$

Here, the Fisher Information is equal to the negative expected value of the second derivative of the log-likelihood, reflecting how the curvature impacts the amount of information.

## Jeffreys Prior: Basis and Significance

The **Jeffreys Prior** [[Jeffreys Prior]] is a non-informative prior that aims to provide a way of assigning prior probabilities that is invariant under reparameterization of the model. It is defined as:

$$
\pi(\theta) \propto \sqrt{I(\theta)}
$$

where $I(\theta)$ is the Fisher Information.

### Why Use Fisher Information for Jeffreys Prior?

1. **Invariance**: Since Fisher Information is derived from the likelihood function, using it as the basis for the Jeffreys prior ensures that the prior is invariant under transformations of the parameter space. This means that regardless of how we parameterize the model, the prior information remains consistent.

2. **Information Content**: The use of the square root of the Fisher Information provides a prior that reflects the amount of information available about the parameter. If a parameter is more sensitive to changes (higher Fisher Information), the Jeffreys prior will assign it more weight.

3. **Improper Priors**: Jeffreys Prior often results in improper priors (those that do not integrate to one), but they can still yield proper posterior distributions when combined with sufficient data. This property allows researchers to conduct Bayesian inference even when the prior itself is not proper.

4. **Objective Bayesian Approach**: The Jeffreys prior is a cornerstone of **objective Bayesian** statistics, where the goal is to minimize the amount of subjective information entered through the prior. By relying on Fisher Information, Jeffreys Prior allows the data to dominate the inference process.

### Examples

- For the normal distribution, the Fisher Information for the mean $\mu$ (with known variance $\sigma^2$) is given by $I(\mu) = \frac{1}{\sigma^2}$. Thus, the Jeffreys prior for $\mu$ is uniform:

  $$
  \pi(\mu) \propto 1
  $$

- For the variance $\sigma^2$, the Fisher Information is  $I(\sigma^2) = \frac{1}{2\sigma^4}$. The Jeffreys prior for $\sigma^2$ is:

  $$
  \pi(\sigma^2) \propto \frac{1}{\sigma^2}
  $$

- For a Bernoulli distribution with parameter $\theta$, the Fisher Information is $I(\theta) = \frac{1}{\theta(1-\theta)}$, leading to the Jeffreys prior:

  $$
  \pi(\theta) \sim \text{Beta}\left(\frac{1}{2}, \frac{1}{2}\right)
  $$

## Conclusion

Fisher Information plays a critical role in the construction of the Jeffreys Prior, allowing for an objective, invariant way of expressing prior beliefs about parameters in statistical models. By using Fisher Information as a basis, the Jeffreys Prior captures the inherent information provided by the data, thereby enabling robust Bayesian inference. The connection to the curvature of the likelihood function underscores the relationship between information and the sensitivity of the likelihood to parameter changes.


# Invariance Under Reparameterization


When we say that a prior (or any statistical method) is **invariant under reparameterization**, we mean that the conclusions drawn from the analysis remain consistent, regardless of how the parameters are expressed or transformed. In other words, if we change the way we parameterize the model, the posterior distribution or the inferences should not change fundamentally.

This property is crucial in Bayesian statistics because it ensures that the choice of prior is not dependent on the specific parameterization used, allowing for a more objective and universally applicable analysis.

## Example of Invariance Under Reparameterization

### Case Study: Normal Distribution

Consider the following scenario involving a normal distribution with unknown mean $\mu$ and known variance $\sigma^2$.

1. **Original Parameterization**:
   - We use a prior $\pi(\mu)$ that is uniform, say:

   $$
   \pi(\mu) \propto 1
   $$

   This indicates that we have no prior preference for any value of $\mu$.

2. **Reparameterized Model**:
   - Now, let's say we want to reparameterize the model in terms of $\tau = \frac{1}{\sigma^2}$  (the precision). This means our likelihood function now depends on $\tau$ instead of $\mu$.

   In this case, we can express $\mu$ as:

   $$
   \mu = \theta + \frac{1}{\tau} z
   $$

   where $z$ is some normally distributed variable. 

3. **Using a Jeffreys Prior**:
   - The Jeffreys prior for the mean $\mu$ is invariant under this transformation because it is defined in terms of Fisher Information. Since the Fisher Information captures how much information about $\theta$ is present, it will yield the same prior beliefs when reparameterized.

   - For $\tau$, the Jeffreys prior would be:

   $$
   \pi(\tau) \propto 1
   $$

### Conclusion of the Example

Even though we changed our parameterization from $\mu$ to $\tau$, the Jeffreys prior remains consistent in its information content. This invariance under reparameterization means that, regardless of whether we model the data using the mean $\mu$ or the precision $\tau$, our conclusions about the parameter remain the same.

## Importance of Invariance

The invariance under reparameterization ensures that our statistical analyses are robust and not unduly influenced by the choice of parameterization. It allows us to focus on the underlying relationships in the data rather than getting caught up in the specifics of how we choose to model it.
