# Part4. Zoo of Distributions in Bayesian analysis

##  Probability Distributions for likelihood

It’s important to understand the broader landscape of probability distributions and their applications in Bayesian modeling.

Distributions describe the possible values a random variable can take and the likelihood of each value. They serve as the foundation for modeling uncertainty in observed data and parameters.  
Depending on the type of variable (continuous or discrete) and its statistical behavior (e.g., skewness, overdispersion, zero-inflation), different distributions are appropriate.

---

## Classification of Common Distributions

| **Distribution**          | **Type**        | **Support**                        | **Typical Use**                                                                 | **Parameters**                          |
|----------------------------|-----------------|------------------------------------|----------------------------------------------------------------------------------|------------------------------------------|
| **Bernoulli**             | Discrete        | 0, 1                               | Binary outcomes (success/failure, yes/no)                                        | $p \in [0,1]$                      |
| **Binomial**              | Discrete        | 0, …, n                            | Number of successes in  $n$trials                                            | $n, p$                             |
| **Poisson**               | Discrete        | 0, 1, 2, …                         | Count data (number of events per unit time or space)                             | $\lambda > 0$                      |
| **Negative Binomial**     | Discrete        | 0, 1, 2, …                         | Overdispersed count data (variance > mean)                                       | $r, p$ or $\mu, \alpha$        |
| **Zero-Inflated Poisson** | Discrete        | 0, 1, 2, …                         | Count data with excess zeros (e.g., fMRI event counts, rare behavioral errors)    | $\lambda, \pi_0$                   |
| **Zero-Inflated NegBin**  | Discrete        | 0, 1, 2, …                         | Overdispersed count data with excess zeros                                       | $\mu, \alpha, \pi_0$               |
| **Uniform**               | Continuous      | [a, b]                             | Flat prior, non-informative range assumption                                     | $a, b$                             |
| **Normal**                | Continuous      | $(-\infty, \infty)$           | Symmetric, continuous outcomes; central in Bayesian inference                     | $\mu, \sigma^2$                    |
| **Exponential**           | Continuous      | $(0, \infty)$                 | Time-to-event or waiting-time processes                                          | $\lambda > 0$                      |
| **Gamma**                 | Continuous      | $(0, \infty)$                | Positive-valued skewed data; priors for rates and variances                      | $\alpha, \beta$                    |
| **Beta**                  | Continuous      | $(0,1)$                      | Proportions and probabilities; conjugate prior for Bernoulli/Binomial likelihoods | $\alpha, \beta$                    |
| **Log-Normal**            | Continuous      | $(0, \infty)$                | Multiplicative processes, skewed positive data                                   | $\mu, \sigma^2$                    |
| **Student’s t**           | Continuous      | $(-\infty, \infty)$          | Data with heavier tails than Normal                                              | $\mu, \sigma^2, \nu$               |
| **Exponential Gaussian**  | Continuous      | $(-\infty, \infty)$          | Mixture of exponential and normal; modeling latency or response times             | $\mu, \sigma, \lambda$             |

---

## Extended Distributions

### Exponential Gaussian (ExGaussian)

The **Exponential Gaussian** (or **ExGaussian**) combines a normal and exponential process, modeling a random variable as:

$$
X = Y + Z, \quad \text{where } Y \sim \mathcal{N}(\mu, \sigma^2), \; Z \sim \text{Exp}(\lambda)
$$

It captures data that show a **normal-like peak with a positive skew**, often observed in **reaction time distributions** or other processes where a baseline Gaussian variability is followed by exponential delay.

**Applications:**
- Reaction time and decision latency modeling  
- Cognitive process modeling (drift-diffusion approximations)  
- Asymmetric response distributions in neuroscience and psychology

**Key properties:**
- Mean: $E[X] = \mu + \frac{1}{\lambda}$
- Variance: $\text{Var}(X) = \sigma^2 + \frac{1}{\lambda^2}$
- Skewness increases as $\lambda$ decreases (i.e., longer exponential tail)

---

### Zero-Inflated Processes

Many real-world count data contain more zeros than expected under a standard Poisson or Negative Binomial model.  
**Zero-Inflated models** explicitly account for this by mixing two components:

1. A *point mass* at zero (extra zeros)  
2. A *count process* (e.g., Poisson or Negative Binomial)

$$
P(Y = 0) = \pi_0 + (1 - \pi_0) f(0 \mid \theta), \quad
P(Y = y>0) = (1 - \pi_0) f(y \mid \theta)
$$

where $\pi_0$ represents the probability of an *excess* zero.

**Applications:**
- Brain imaging: voxels with no detected signal
- Behavioral data: participants who never respond
- Ecological counts: many zero observations due to absence of species

**Variants:**
- Zero-Inflated Poisson (ZIP)  
- Zero-Inflated Negative Binomial (ZINB)

**Advantages:**
- Better fit for sparse count data  
- Reduced bias in mean and variance estimates  
- Straightforward Bayesian implementation via mixture priors

---

## Common Prior Distributions for Model Parameters

In Bayesian models, prior distributions express our beliefs about parameters **before observing data**.  
The choice of prior should reflect both **theoretical expectations** and **mathematical constraints** on the parameter — for example, whether it is positive, bounded between 0 and 1, or can take any real value.

Choosing appropriate priors is essential to ensure that:
- parameters remain within valid ranges (e.g., variance > 0, probability in [0,1]);
- inference remains stable and interpretable;
- the model can incorporate both informative and weakly informative assumptions.

---

## Overview of Common Priors

| **Prior Distribution** | **Parameter Type** | **Support (Range)** | **Typical Use** | **Key Parameters** | **Notes / Example Usage** |
|-------------------------|--------------------|----------------------|-----------------|--------------------|-----------------------------|
| **Normal**              | Unbounded (real)   | $(-\infty, \infty)$ | Location parameters (means, regression coefficients) | $\mu, \sigma$ | Centered around expected mean; symmetric and simple. Often used as weakly informative prior with large \(\sigma\). |
| **Beta**                | Bounded (0, 1)     | $(0,1)$ | Probabilities, proportions, mixture weights | $\alpha, \beta$ | Flexible shape: uniform (\(\alpha=\beta=1\)), U-shaped, or peaked. Conjugate to Bernoulli/Binomial likelihoods. |
| **Exponential**         | Positive           | $(0,\infty)$ | Scale, rate, or precision parameters | $\lambda$ | Enforces positivity. Common weakly-informative prior for standard deviations or rates. |
| **Gamma**               | Positive           | $(0,\infty)$ | Rate, precision, or variance components | $\alpha, \beta$ | More flexible than Exponential. Conjugate for many exponential-family likelihoods. |
| **Inverse-Gamma**       | Positive           | $(0,\infty)$ | Variance or noise-level parameters | $\alpha, \beta$ | Heavy-tailed, ensures positivity. Conjugate prior for variance in Normal models. |
| **Chi-squared**         | Positive           | $(0,\infty)$ | Variance, scale, or precision parameters | $\nu$ (degrees of freedom) | Often used as a prior for variance-like quantities or in hierarchical models (scaled form). |
| **Uniform**             | Bounded            | $[a,b]$ | Parameters with known bounds | $a, b$ | Non-informative but can lead to improper posteriors if not carefully constrained. |

---


The most important rule in prior selection is **compatibility with the parameter domain**: