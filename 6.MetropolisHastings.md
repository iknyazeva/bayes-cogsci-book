
# Metropolis-Hastings Algorithm

The **Metropolis-Hastings algorithm** is a Markov Chain Monte Carlo (MCMC) method that allows us to sample from a probability distribution, even when we only know it up to a normalizing constant. This algorithm is particularly useful when the probability distribution is complex or difficult to sample directly.

## Overview

Suppose we have a target distribution $p(\theta)$ that we wish to sample from, but we only know it up to a normalizing constant . That is, we know a function $g(\theta)$ proportional to $p(\theta)$:

$$
p(\theta) \propto g(\theta)
$$

The normalizing constant might be difficult to compute due to complex integrals.

The **Metropolis-Hastings algorithm** constructs a Markov chain whose stationary distribution is the target distribution $p(\theta)$. The basic idea is to iteratively propose a new value for $\theta$, accept or reject the proposed value based on an acceptance ratio, and use this process to generate a sequence of samples that approximates the target distribution.

### Steps of the Metropolis-Hastings Algorithm

The algorithm proceeds as follows:

1. **Initialize:** Choose an arbitrary starting value $\theta_0$.
   
2. **Iterate for a large number of steps (i = 1 to m):**
    1. **Propose a candidate:** Draw a candidate $\theta^*$ from a proposal distribution $q(\theta^* | \theta_{i-1})$, where $q$ is easy to sample from.
    
    2. **Compute the acceptance ratio:** Calculate the acceptance ratio $\alpha$, which is given by:
    
    $$
    \alpha = \frac{g(\theta^*) \cdot q(\theta_{i-1} | \theta^*)}{g(\theta_{i-1}) \cdot q(\theta^* | \theta_{i-1})}
    $$
    
    3. **Accept or reject the candidate:** 
        - If $\alpha \geq 1$ , accept the candidate, i.e., set $\theta_i = \theta^*$.
        - If $\alpha < 1$, accept the candidate with probability $\alpha$;  otherwise, reject it and set $\theta_i = \theta_{i-1}$.
    
    These steps ensure that even if the proposal distribution $q$ is not the same as the target distribution $p$, the chain will converge to the correct distribution over time.

### Markov Chain

The sequence of samples $\theta_0, \theta_1, \theta_2, \dots$ generated by this algorithm forms a **Markov chain**. The key property of a Markov chain is that the probability of transitioning to the next state depends only on the current state, not on the history of previous states. This memoryless property makes the Metropolis-Hastings algorithm efficient for sampling from complex distributions.

### Choice of Proposal Distribution

A critical decision in the Metropolis-Hastings algorithm is the choice of the proposal distribution $q(\theta^* | \theta_{i-1})$. 

#### Symmetric Proposal Distributions

In some cases, the proposal distribution is symmetric, i.e.,

$$
q(\theta^* | \theta_{i-1}) = q(\theta_{i-1} | \theta^*)
$$

In this case, the acceptance ratio simplifies to:

$$
\alpha = \frac{g(\theta^*)}{g(\theta_{i-1})}
$$

This occurs, for example, in **random walk Metropolis-Hastings**, where $q$ is often a normal distribution centered at the previous value $\theta_{i-1}$.

#### Random Walk Metropolis-Hastings

In the **random walk Metropolis-Hastings** algorithm, the candidate is drawn from a normal distribution:

$$
q(\theta^* | \theta_{i-1}) \sim \mathcal{N}(\theta_{i-1}, \sigma^2)
$$

where $\sigma^2$ is the variance of the normal distribution. Since the normal distribution is symmetric, the acceptance ratio reduces to:

$$
\alpha = \frac{g(\theta^*)}{g(\theta_{i-1})}
$$

The parameter $\sigma^2$ affects the step size of the random walk. If the step size is too small, the chain will take many small steps and explore the space slowly. If the step size is too large, the chain will propose values far from the current state, which may be rejected frequently, leading to inefficiency.

### Acceptance Rate

The performance of the Metropolis-Hastings algorithm depends on the acceptance rate. A high acceptance rate suggests that the proposal distribution $q$ is close to the target distribution $p$. However, in the case of **random walk Metropolis-Hastings**, a high acceptance rate may indicate that the steps are too small, causing the chain to move slowly.

An optimal acceptance rate for the random walk Metropolis-Hastings algorithm is typically between 23% and 50%.

### Conclusion

The Metropolis-Hastings algorithm is a powerful tool for sampling from complex distributions. By carefully choosing the proposal distribution $q$ and tuning the parameters, we can ensure that the Markov chain efficiently explores the target distribution. In the next section, we will demonstrate this algorithm in a simple discrete case and show that the Markov chain converges to the target distribution.


# Example (discrete case)


## Problem Setup

Let's consider an example where someone has a loaded coin that lands heads 70% of the time. They offer you this coin (or perhaps a fair one) and let you flip it five times. You observe 2 heads and 3 tails. Now, you need to estimate the posterior probability that the coin is the loaded one.

### Parameters:
- **Unknown parameter $\theta$**: Can be either "fair" or "loaded."
- **Prior probability** that the coin is loaded: $P(\theta = \text{loaded}) = 0.6$.
- **Likelihood**: The likelihood depends on whether the coin is fair or loaded and follows a binomial distribution based on the observed heads and tails.
  
Given 2 heads and 3 tails from 5 flips, we can compute the posterior distribution using Bayes' Theorem:

$$
P(\theta \mid x = 2) \propto \text{Likelihood} \times \text{Prior}
$$

### Prior and Likelihood
- Prior for $\theta = \text{loaded}$: $P(\theta = \text{loaded}) = 0.6$
- Prior for $\theta = \text{fair}$: $P(\theta = \text{fair}) = 0.4$
- Likelihood  for binomial($f(x \mid \theta = \theta) = \binom{n}{x} (\theta)^x (1-\theta)^{n - x}$
- Likelihood for $x = 2 \mid \theta = \text{loaded}$: $\binom{5}{2} \times (0.7)^2 \times (0.3)^3$
- Likelihood for $x = 2 \mid \theta = \text{fair}$: $\binom{5}{2} \times (0.5)^2 \times (0.5)^3$

### Posterior Calculation
The posterior probability is proportional to the likelihood multiplied by the prior. In this case, the posterior for $\theta = \text{loaded}$ is approximately $0.388$, and for $\theta = \text{fair}$, it is $0.612$. These results can be computed in closed form using the binomial distribution.

$$
P(\theta \mid x = 2) = \frac{f( x = 2\mid  \theta)f(\theta)}{f(x)} = \frac{0.5^5*0.4\times I_{\text{faired}}+(0.7)^2 \times (0.3)^3\times0.6I_{\text{loaded}}}{0.5^5\times0.4+(0.7)^2 \times (0.3)^3\times0.6} 
$$

$$
P(\theta \mid x = 2) = 0.612\times I_{\text{faired}}+0.388\times I_{\text{loaded}}
$$

However, what if the problem were more complicated and we couldn't compute the posterior analytically? In such cases, we can use **MCMC** to estimate the posterior probabilities.

## Using the Metropolis-Hastings Algorithm

We can use the Metropolis-Hastings algorithm to set up a Markov chain that moves between the states $\theta = \text{fair}$ and $\theta = \text{loaded}$, with transition probabilities that depend on the likelihood and prior. The goal is to estimate the posterior distribution by simulating from this Markov chain.

### Steps in the Algorithm

1. **Initialize the chain**: Start at an arbitrary state, either $\theta_0 = \text{fair}$ or $\theta_0 = \text{loaded}$.
2. **Propose a new state**: In each iteration $i$, propose a new state $\theta^*$. If the current state is fair, propose $\theta^* = \text{loaded}$, and vice versa.
3. **Compute the acceptance probability $\alpha$**:
   
   The acceptance ratio is given by:
   
$$
   \alpha = \frac{P(x = 2 \mid \theta^*) q(\theta^*)}{P(x = 2 \mid \theta_{i-1}) q(\theta_{i-1})}
$$

   In this example, the proposal distribution $q(\theta^* \mid \theta_{i-1})$ is symmetric, so it cancels out in the acceptance ratio.

5. **Acceptance or Rejection**: 
   - If $\alpha \geq 1$, accept the new state $\theta^*$.
   - If $\alpha < 1$, accept the new state with probability $\alpha$, otherwise stay in the current state.

### Example Calculation

Let's compute $\alpha$ for transitioning between "fair" and "loaded" states:
- When proposing to move to $\theta^* = \text{loaded}$:
  
$$
  \alpha = \frac{0.7^2 \times 0.3^3 \times 0.6}{0.5^5 \times 0.4} = 0.635
$$
 
- When proposing to move to $\theta^* = \text{fair}$:
  
$$
  \alpha = \frac{0.5^5 \times 0.4}{0.7^2 \times 0.3^3 \times 0.6} = 1.574
$$
  
Thus, when moving to "fair," we always accept (since $\alpha > 1$), and when moving to "loaded," we accept with probability $0.635$.

### Simulating the Chain

Using the transition probabilities, we can simulate a Markov chain with two states:
- If the chain is in the "fair" state, it stays there with probability $0.365$, or moves to "loaded" with probability $0.635$.
- If the chain is in the "loaded" state, it moves to "fair" with probability 1 (since we always propose the other state).

### Conclusion

By simulating this Markov chain over many iterations, we can estimate the posterior probability that the coin is loaded. Over time, the chain will spend approximately 38.8% of the time in the "loaded" state, which corresponds to the posterior probability $P(\theta = \text{loaded} \mid x = 2)$.

This example demonstrates how the **Metropolis-Hastings algorithm** can be used to approximate posterior probabilities even when the exact posterior cannot be computed analytically.


## Example (continues case) Personnel data

# Tutorial: Examples of Models Without Nice, Clean Posterior Distributions


### Problem Setup

Suppose we have data representing the percentage change in total personnel from last year to this year for ten companies in a particular industry. Assume these values are independent measurements from a **normal distribution** with:
- **Known variance**: 1  
- **Unknown mean**: $\mu$  

We model this as:

$$
Y_i \mid \mu \sim \mathcal{N}(\mu, 1)
$$

Here, $\mu$ represents the average growth rate for the industry. A small variance suggests a stable industry.

### Prior Distribution

The conjugate prior for  $\mu$ would typically be a normal distribution. However, let's use a **t-distribution** as the prior for $\mu$, specifically:
$$
\mu \sim t(0, 1, 1)
$$
This t-distribution:
- Is centered at $0$, implying a 50% chance for positive or negative growth.
- Has heavier tails than a normal distribution, allowing for extreme values.

### Posterior Distribution

The posterior distribution is proportional to the **likelihood** times the **prior**:

$$
p(\mu \mid \mathbf{y}) \propto \text{Likelihood} \times \text{Prior}
$$

#### Likelihood
For the normal distribution:

$$
\text{Likelihood} = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y_i - \mu)^2}
$$

#### Prior
For the t-distribution:

$$
\text{Prior} = \frac{1}{\pi (1 + \mu^2)}
$$

Combining these:

$$
p(\mu \mid \mathbf{y}) \propto \prod_{i=1}^{n} e^{-\frac{1}{2}(y_i - \mu)^2} \cdot \frac{1}{1 + \mu^2}
$$

### Simplifying the Expression

1. Drop constant terms (e.g., $\sqrt{2\pi}$ and $\pi$ as we're working up to proportionality.
2. Combine terms:

$$
p(\mu \mid \mathbf{y}) \propto e^{-\frac{1}{2} \sum_{i=1}^{n} (y_i - \mu)^2} \cdot \frac{1}{1 + \mu^2}
$$

This posterior is almost proportional to a normal distribution but includes the term $1 + \mu^2$ in the denominator, complicating its form. It doesn't match any standard distribution, so we'll need alternative computational techniques to analyze it.

