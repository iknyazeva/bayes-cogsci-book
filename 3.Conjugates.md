# Part 3 Discrete model. Conjugates and Predictive Distributions

Understanding discrete models such as the Binomial and Poisson distributions is essential because they describe count-based events â€” successes in a fixed number of trials or occurrences within a given time or space. Many real-world processes, from behavioral responses to neural spike counts, follow discrete patterns rather than continuous ones. By studying their conjugate relationships, we can perform Bayesian updating analytically and gain intuition about how evidence modifies our beliefs. Moreover, discrete models offer a clear, interpretable setting for exploring the logic of prior, likelihood, and predictive distributions before moving to more complex continuous cases.

## Conjugate Priors
A **conjugate prior** is a prior distribution that, when combined with a particular likelihood function, produces a posterior distribution of the same family as the prior.  
- Example: A **Beta prior** with a **Binomial likelihood** results in a **Beta posterior**.  
- Advantage: Makes Bayesian updating mathematically simple and analytically tractable.

## Conjugate Distributions
The pair of likelihood and prior that lead to this convenient property are called **conjugate distributions**.  
- For exponential family distributions (e.g., Binomial, Poisson, Normal), conjugate priors often exist.  
- This property helps avoid complex integrals when calculating posteriors.

## Predictive Prior Distribution
The **prior predictive distribution** represents the distribution of future or new data *before* observing any evidence.  
- It is obtained by integrating the likelihood over the prior distribution:  

$$
f(y) = \int f(y \mid \theta) \, f(\theta) \, d\theta
$$

- This captures uncertainty in both the parameter and the data.

## Predictive Posterior Distribution
The **posterior predictive distribution** describes the distribution of new data *after* observing evidence.  
- It integrates the likelihood over the **posterior distribution**:  

$$
f(y_{\text{new}} \mid y) = \int f(y_{\text{new}} \mid \theta) \, f(\theta \mid y) \, d\theta
$$

- It combines updated beliefs about parameters (posterior) with possible outcomes of new observations.

---

**In summary**:
- **Conjugate priors** make Bayesian updating easy.  
- **Conjugate distributions** are the matched prior-likelihood pairs.  
- **Prior predictive** shows data patterns before observing evidence.  
- **Posterior predictive** shows expected new data after incorporating evidence.
