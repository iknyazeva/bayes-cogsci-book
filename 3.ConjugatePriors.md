---
tags:
  - priors
  - bayes_course
---

## What is a Conjugate Prior?

Suppose we have a parameter $\theta$ (considered as a random variable) and an observation $x$  According to Bayes' theorem, the **posterior distribution** of $\theta$ given $x$ is calculated as follows:

$$
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int_{\theta} p(x|\theta)p(\theta) d\theta}
$$

Where:

- $p(\theta)$ is the **prior distribution**, representing our initial belief about the parameter $\theta$.
- $p(x|\theta)$ is the **likelihood**, indicating how likely the observed data $x$ is given $\theta$.

If the resulting **posterior distribution** $p(\theta|x)$ belongs to the same family of probability distributions as the prior distribution $p(\theta)$ (i.e., it has the same form but with updated parameters), then this family of distributions is called the **conjugate family** with respect to the likelihood $p(x|\theta)$. In this case, the prior distribution $p(\theta)$ is said to be a **conjugate prior** for the likelihood function $p(x|\theta)$.

In Bayesian statistics, the concept of a **conjugate prior** and **conjugate family of distributions** are fundamental. These concepts simplify the process of updating beliefs about a parameter in light of new data. 

A family of distributions is called conjugate if using a prior from this family results in a posterior that also belongs to the same family. For example:

- The Beta distribution is conjugate to the Bernoulli and binomial distributions.
- The Gamma distribution is conjugate to the Poisson distribution

### Why Use Conjugate Priors?

Conjugate priors make life simpler when calculating posteriors. Without them, the denominator in Bayes' theorem might involve complex integrals. Using conjugate families allows us to achieve **closed-form solutions** for the posterior distribution.  The conjugate family simplifies posterior computations, avoiding intractable integrals.



#  Beta and Bernoulli Conjugate Priors

## Understanding the Relationship Between Beta and Bernoulli

The **uniform distribution** is a special case of the Beta distribution with parameters $\text{Beta}(1, 1)$. More generally, any Beta distribution is conjugate to the Bernoulli distribution. This means that if you start with a Beta prior and observe Bernoulli data, you will obtain a Beta posterior. This property simplifies Bayesian analysis significantly.

### Beta Distribution

The Beta distribution for a parameter $\theta$ over the interval $[0, 1]$ is given by:

$$
f(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
$$

where:

- $\alpha > 0$ and $\beta > 0$ are shape parameters.
- $\Gamma(\cdot)$ is the Gamma function.

### Bernoulli Likelihood and Beta Prior

For a Bernoulli likelihood with observations $y_1, y_2, \ldots, y_n$, if we use a Beta prior $\text{Beta}(\alpha, \beta)$, the posterior distribution for $\theta$ will also be a Beta distribution:

$$
\theta \mid y \sim \text{Beta}(\alpha + \sum y_i, \beta + n - \sum y_i)
$$

This shows how both the prior and the data contribute to the posterior distribution.

###  Posterior Calculation

The posterior distribution is proportional to the product of the likelihood and the prior. For the Bernoulli likelihood and Beta prior:

$$
f(\theta \mid y) \propto \theta^{\sum y_i} (1 - \theta)^{n - \sum y_i} \cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
$$

Simplifying, we obtain:

$$
f(\theta \mid y) \propto \theta^{\alpha + \sum y_i - 1} (1 - \theta)^{\beta + n - \sum y_i - 1}
$$

This is a Beta distribution with parameters $\alpha + \sum y_i$ and $\beta + n - \sum y_i$.



## Using Conjugate Priors with Poisson Data


###  Poisson Data

Consider a scenario where we're interested in modeling the number of chocolate chips in cookies. In mass-produced chocolate chip cookies, a large amount of dough is prepared, and chocolate chips are mixed in. This dough is then divided into individual cookies. If we assume the chocolate chips are evenly distributed and don't take up significant volume, the number of chips per cookie would follow a **Poisson distribution**.

### Poisson Distribution and Likelihood

We can model the number of chocolate chips per cookie $Y_i$ as a Poisson random variable with parameter $\lambda$, which represents the average number of chips per cookie. For a set of observations $y_1, y_2, \ldots, y_n$, the likelihood function given $\lambda$ is:

$$
L(\lambda | y) = \lambda^{\sum_{i=1}^{n} y_i} e^{-n\lambda} \prod_{i=1}^{n} \frac{1}{y_i!}
$$

where:

- $\lambda > 0$ is the rate parameter.
- $\sum_{i=1}^{n} y_i$ is the total number of chips observed across all cookies.
- $n$ is the number of observations.

### Choosing a Prior for $\lambda$

Since $\lambda$ must be positive, we choose a prior distribution that is defined for positive values. A natural choice is the **Gamma distribution**, which is conjugate to the Poisson distribution.


The **Gamma distribution** is defined as:

$$
f(\lambda | \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}, \quad \lambda > 0
$$

where:

- $\alpha$ and $\beta$ are shape and rate parameters, respectively.
- $\Gamma(\alpha)$ is the gamma function.

### Posterior Distribution

The posterior distribution is obtained by multiplying the likelihood by the prior. This gives:

$$
f(\lambda | y) \propto \lambda^{\sum_{i=1}^{n} y_i + \alpha - 1} e^{-(n + \beta) \lambda}
$$

This is also a Gamma distribution with updated parameters:

$$
\lambda \sim \text{Gamma} \left( \alpha + \sum_{i=1}^{n} y_i, \beta + n \right)
$$

### Posterior Mean

The mean of a Gamma random variable is given by $\frac{\alpha}{\beta}$. So, the posterior mean is:

$$
\text{Posterior Mean} = \frac{\alpha + \sum_{i=1}^{n} y_i}{\beta + n}
$$

This can be rewritten as a weighted average of the prior mean and the data mean:

$$
\text{Posterior Mean} = \frac{\beta}{\beta + n} \cdot \frac{\alpha}{\beta} + \frac{n}{\beta + n} \cdot \frac{\sum_{i=1}^{n} y_i}{n}
$$

where:

- $\frac{\alpha}{\beta}$ is the prior mean.
- $\frac{\sum_{i=1}^{n} y_i}{n}$ is the data mean.

## Choosing Hyperparameters $\alpha$ and $\beta$

### Strategy 1: Use Prior Knowledge

1. **Prior Mean**: Set based on prior knowledge or belief about the number of chips per cookie.
2. **Prior Variance**: Set based on the uncertainty in our belief. For the Gamma distribution, the variance is $\frac{\alpha}{\beta^2}$.

Given these two values, we can solve for $\alpha$ and $\beta$.

### Strategy 2: Effective Sample Size

- The effective sample size for the prior is $\beta$. We can set $\beta$ based on how confident we are in our prior knowledge relative to the data size.

## Using Vague Priors

If we want to represent ignorance (lack of prior knowledge), we can use a vague prior:

1. **Vague Gamma Prior**: Set $\alpha = \epsilon$ and $\beta = \epsilon$, where $\epsilon$ is a small positive value.

2. **Posterior Under Vague Prior**:

$$
\text{Posterior Mean} = \frac{\epsilon + \sum_{i=1}^{n} y_i}{\epsilon + n}
$$

For very small $\epsilon$, the posterior mean approximates the data mean, meaning the prior has minimal influence.

## Conclusion

Using conjugate priors, like the Gamma prior for Poisson data, simplifies Bayesian updating by providing closed-form solutions for the posterior distribution. This allows for easy calculation of the posterior mean and understanding the influence of the prior relative to the data.

# Hierarchical Models

We can represent our model hierarchically:

1. **Observations**: $Y_1, \ldots, Y_n$ follow a Bernoulli likelihood with parameter $\theta$.
2. **Parameter $\theta$**: Follows a Beta prior, $\theta \sim \text{Beta}(\alpha, \beta)$.
3. **Hyperparameters**: $\alpha$ and $\beta$ are the shape parameters of the Beta distribution, which may also have their own priors (known as **hyperpriors**).

### Example

If we set $\alpha = \alpha_0$ and $\beta = \beta_0$, we are specifying fixed values for the prior. In more complex problems, we might place priors on $\alpha$ and $\beta$ themselves, creating additional levels in the hierarchy.

### Benefits and Limitations

- **Flexibility**: Hierarchical models provide greater flexibility, especially in complex problems where we need to capture additional uncertainty.
- **Complexity**: For simpler problems, adding more hierarchical levels may not add much value and can complicate analysis unnecessarily.




## 5. Conclusion

The use of conjugate priors, especially the Beta distribution for Bernoulli and binomial likelihoods, allows for straightforward and tractable posterior updates. This property is particularly useful in Bayesian analysis, enabling closed-form solutions and easy interpretation of results.

Understanding and leveraging conjugate families, as well as hierarchical models, are key tools for effective Bayesian modeling and inference.
