# Gibbs Sampling Tutorial

So far, it was demonstrated how to sample for just one single parameter. But what happens if we seek the posterior distribution of multiple parameters, and that posterior distribution doesn't have a standard form? One option is to perform Metropolis-Hastings by sampling candidates for all the parameters at once and accepting or rejecting all those candidates together. While this is possible, it can get complicated. 

A simpler option is to sample the parameters one at a time.

## Gibbs Sampling: An Overview

Suppose we have a joint posterior distribution for two parameters, $\theta$ and $\phi$, given our data $Y$. Let's also assume we only know this distribution up to proportionality, meaning we are missing the normalizing constant. What we've calculated is a function $g(\theta, \phi)$.

If we knew the value of $\phi$, we could draw a candidate for $\theta$ and use this $g$-function to compute the Metropolis-Hastings ratio and possibly accept the candidate. Similarly, if we don't know the value of $\phi$, we would perform an update for it by drawing a candidate using some proposal distribution. We would plug in the current value of $\theta$ from the Markov chain to compute the Metropolis-Hastings ratio. 

Once we've drawn both $\theta$ and $\phi$, that completes one iteration. We then begin the next iteration by drawing a new $\theta$. In other words, we go back and forth, updating the parameters one at a time, plugging the current value of the other parameter into the $g$-function. 

This idea of one-at-a-time updates is the core of **Gibbs Sampling**, which produces a stationary Markov chain whose stationary distribution is the target or posterior distribution.

## Full Conditionals and Factorization

Before describing the full Gibbs sampling algorithm, there's one more key insight we can use: **the chain rule of probability**. We know that the joint posterior distribution of $\theta$and $\phi$ can be factored into:

$$
p(\theta, \phi | Y) = p(\phi | Y) \cdot p(\theta | \phi, Y)
$$

Here, $p(\theta | \phi, Y)$ is the **full conditional distribution** for $\theta$, given $\phi$ and the data. Similarly, $p(\phi | \theta, Y)$ is the full conditional for $\phi$, given $\theta$ and $Y$.

### Why Use Full Conditionals?

In some cases, the full conditional distribution is a standard distribution that we know how to sample from directly. If this is the case, we no longer need to draw candidates and decide whether to accept them. If we treat the full conditional as the proposal distribution, the Metropolis-Hastings acceptance probability becomes exactly one. This makes Gibbs sampling more efficient.

To summarize:

- We start from the **full posterior distribution**.
- The process of finding full conditional distributions is akin to finding the posterior of each parameter, treating all other parameters as known constants.

## Gibbs Sampling Algorithm

Let's now describe the Gibbs sampling algorithm using the example of two parameters, $\theta$ and $\phi$.

1. **Initialization**: Draw initial values for $\theta_0$ and $\phi_0$.

2. **Iteration**: For $i = 1$ to $M$, repeat the following steps:
   - Draw $\theta_i$ from its full conditional distribution $p(\theta | \phi_{i-1}, Y)$, using the previous iteration's draw of $\phi$, denoted $\phi_{i-1}$.
   - Draw $\phi_i$ from its full conditional distribution $p(\phi | \theta_i, Y)$, using the newly drawn $\theta_i$.

   Together, these two steps complete one cycle of the Gibbs sampler and produce a pair $(\theta_i, \phi_i)$. This completes one iteration of the MCMC sampler.

### Extending to More Parameters

If there are more than two parameters, the process is similar. One Gibbs cycle would include an update for each parameter, cycling through them one at a time. 

# Gibbs Sampling: Normal Likelihood with Unknown Mean and Variance

Let's return to the example where we have a normal likelihood with an unknown mean and unknown variance. To remind ourselves, here's the model:

The response for individual $i$, given $\mu$ and $\sigma^2$, is independent and identically distributed (iid) normal, with mean $\mu$and variance $\sigma^2$, for observations $i = 1, 2, ..., n$. We're also going to assume independent priors:
- A normal prior for $\mu$, with hyperparameters $\mu_0$ and $\tau^2$,
- An inverse gamma prior for $\sigma^2$, with hyperparameters $\alpha_0$ and $\beta_0$.

In this case, we chose a normal prior for $\mu$ because when $\sigma^2$is a known constant, the normal distribution is the **conjugate prior** for $\mu$. Similarly, when $\mu$ is known, the inverse gamma is the **conjugate prior** for $\sigma^2$. This gives us convenient full conditional distributions for a Gibbs sampler.

## Joint Posterior Distribution

Let’s work out the form of the **joint posterior distribution**. The joint posterior distribution of $\mu$ and $\sigma^2$, given the data $Y$, is proportional to the joint distribution of everything, which includes:
- The **likelihood**, the joint distribution of the data given the parameters, and
- The **priors** for $\mu$ and $\sigma^2$.

### Likelihood and Priors

Since the observations are independent, we can write their densities as a product of normal densities for each $y_i$, given $\mu$and $\sigma^2$. Then we have:
- The prior for $\mu$, which is a normal density for $\mu$ given its hyperparameters,
- The prior for $\sigma^2$, which is an inverse gamma given its hyperparameters.

The joint posterior distribution is proportional to the product of these densities:

$$
p(\mu, \sigma^2 | Y) \propto \prod_{i=1}^n \text{Normal}(y_i | \mu, \sigma^2) \times \text{Normal}(\mu | \mu_0, \tau^2) \times \text{InverseGamma}(\sigma^2 | \alpha_0, \beta_0)
$$

Skipping the detailed math steps for brevity, we ultimately find the joint posterior distribution without needing the normalizing constants. So we work with the simplified function:

$$
p(\mu, \sigma^2 | Y) \propto \prod_{i=1}^n \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right) \times \exp\left(-\frac{(\mu - \mu_0)^2}{2\tau^2}\right) \times \left(\sigma^2\right)^{-\alpha_0 - 1} \exp\left(-\frac{\beta_0}{\sigma^2}\right)
$$

### Full Conditional Distributions

#### Full Conditional for $\mu$
Let’s assume $\sigma^2$is known. The full conditional distribution of $\mu$, given $\sigma^2$ and $Y$, can be derived by focusing on the terms in the posterior that involve $\mu$. The result is:

$$
p(\mu | \sigma^2, Y) \propto \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu)^2 - \frac{1}{2\tau^2}(\mu - \mu_0)^2\right)
$$

This expression is proportional to a normal distribution. After completing the algebra, we find that $\mu$follows a normal distribution with the following parameters:

$$
\mu | \sigma^2, Y \sim \text{Normal}\left(\frac{\frac{n \bar{y}}{\sigma^2} + \frac{\mu_0}{\tau^2}}{\frac{n}{\sigma^2} + \frac{1}{\tau^2}}, \frac{1}{\frac{n}{\sigma^2} + \frac{1}{\tau^2}}\right)
$$

#### Full Conditional for $\sigma^2$
Now, let’s assume $\mu$ is known. The full conditional distribution of $\sigma^2$, given $\mu$ and $Y$, is proportional to:

$$
p(\sigma^2 | \mu, Y) \propto (\sigma^2)^{-(\alpha_0 + \frac{n}{2} + 1)} \exp\left(-\frac{1}{\sigma^2} \left(\frac{\sum_{i=1}^n (y_i - \mu)^2}{2} + \beta_0\right)\right)
$$

This expression is proportional to an inverse gamma distribution. Therefore, $\sigma^2$ follows an inverse gamma distribution with the following parameters:

$$
\sigma^2 | \mu, Y \sim \text{InverseGamma}\left(\alpha_0 + \frac{n}{2}, \beta_0 + \frac{1}{2}\sum_{i=1}^n (y_i - \mu)^2\right)
$$

## Gibbs Sampler

To implement the Gibbs sampler:
1. **Initialize** $\mu_0$ and $\sigma^2_0$.
2. For $i = 1$ to $M$:
   - Sample $\mu_i$ from $p(\mu | \sigma^2_{i-1}, Y)$,
   - Sample $\sigma^2_i$ from $p(\sigma^2 | \mu_i, Y)$.

By alternating between these two steps, we generate a Markov chain whose stationary distribution is the full posterior distribution for $\mu$ and $\sigma^2$.





## Conclusion

Gibbs sampling is a powerful technique for generating samples from a complex posterior distribution, especially when the full conditional distributions are known or easy to derive. By updating one parameter at a time, we can efficiently explore the posterior, even when direct sampling of the joint posterior is impractical.

In the following segments, we'll provide a concrete example of finding full conditional distributions and constructing a Gibbs sampler.

