
# Linear Regression

## 1. Brief Review of Regression

Recall that linear regression is a model for predicting a response or dependent variable ($Y$, also called an output) from one or more covariates or independent variables ($X$, also called explanatory variables, inputs, or features). For a given value of a single $x$, the expected value of $y$ is

$$
E[y] = \beta_0 + \beta_1 x
$$

or we could say that $Y \sim N(\beta_0 + \beta_1 x, \sigma^2)$. For data $(x_1, y_1), \dots, (x_n, y_n)$, the fitted values for the coefficients, $\hat{\beta_0}$ and $\hat{\beta_1}$, are those that minimize the sum of squared errors:

$$
\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

where the predicted values for the response are $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$. These fitted coefficients give the least-squares line for the data.

This model extends to multiple covariates, with one $\beta_j$ for each of the $k$ covariates:

$$
E[y_i] = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}
$$

Optionally, we can represent the multivariate case using vector-matrix notation.

## 2. Conjugate Modeling

In the Bayesian framework, we treat the $\beta$ parameters as unknown, put a prior on them, and then find the posterior. We might treat $\sigma^2$ as fixed and known, or we might treat it as unknown and also put a prior on it. Because the underlying assumption of a regression model is that the errors are independent and identically normally distributed with mean zero and variance $\sigma^2$, this defines a normal likelihood.

### 2.1 $\sigma^2$ Known

Sometimes we may know the value of the error variance $\sigma^2$. This simplifies the calculations. The conjugate prior for the $\beta$'s is a normal prior. In practice, people typically use a non-informative prior, i.e., the limit as the variance of the normal prior goes to infinity, which is a completely flat prior and is also the Jeffreys prior. Using this prior gives a posterior distribution for $\beta$ which has the same mean as the standard least-squares estimates. If we are only estimating $\beta$ and treating $\sigma^2$ as known, then the posterior for $\beta$ is a (multivariate) normal distribution.

If we just have a single covariate, then the posterior for the slope is:

$$
\beta_1 \mid y \sim N\left(\frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}, \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right)
$$

If we have multiple covariates, then using matrix-vector notation, the posterior for the vector of coefficients is:

$$
\beta \mid y \sim N\left((X^T X)^{-1} X^T y, (X^T X)^{-1} \sigma^2\right)
$$

where $X$ denotes the design matrix and $X^T$ is the transpose of $X$. The intercept is typically included in $X$ as a column of 1’s. Using an improper prior requires us to have at least as many data points as we have parameters to ensure the posterior is proper.

### 2.2 $\sigma^2$ Unknown

If we treat both $\beta$ and $\sigma^2$ as unknown, the standard prior is the non-informative Jeffreys prior, $f(\beta, \sigma^2) \propto \frac{1}{\sigma^2}$. Again, the posterior mean for $\beta$ will be the same as the standard least-squares estimates. The posterior for $\beta$ conditional on $\sigma^2$ is the same normal distribution as when $\sigma^2$ is known, but the marginal posterior distribution for $\beta$, with $\sigma^2$ integrated out, is a t distribution, analogous to the t-tests for significance in standard linear regression.

The posterior t distribution has mean $(X^T X)^{-1} X^T y$ and scale matrix (related to the variance matrix) $s^2 (X^T X)^{-1}$, where

$$
s^2 = \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{n - k - 1}
$$

The posterior distribution for $\sigma^2$ is an inverse gamma distribution:

$$
\sigma^2 \mid y \sim \text{IG}\left(\frac{n - k - 1}{2}, \frac{n - k - 1}{2} s^2\right)
$$

In the simple linear regression case (single variable), the marginal posterior for $\beta$ is a t distribution with mean:

$$
\frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

and scale $s^2 / \sum_{i=1}^{n}(x_i - \bar{x})^2$.

If we are trying to predict a new observation at a specified input $x^*$, that predicted value has a marginal posterior predictive distribution that is a t distribution, with mean $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x^*$ and scale:

$$
\text{ser} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1) s_x^2}}
$$

$\text{ser}$ is the residual standard error of the regression, which can be found easily. $s_x^2$ is the sample variance of $x$. Recall that the predictive distribution for a new observation has more variability than the posterior distribution for $\hat{y}$, because individual observations are more variable than the mean.

This is the formula for the prediction interval:

$$
\hat{y}^* \pm t_{n-2}^{-1}(0.975) \cdot \text{ser} \cdot \sqrt{1 + \frac{1}{n} + \frac{(x - \bar{x})^2}{(n - 1) s_x^2}}
$$

where:

- $\hat{y}^*$ is the prediction mean ,
- $t_{n-2}^{-1}(0.975)$ is the 0.975 quantile of a standard \(t\) distribution with \(n - 2\) degrees of freedom,
- $n$ is the number of data points,
- $\text{ser}$ is the residual standard error (estimate of $\sigma$),
- $\bar{x}$ is the sample mean of driving distance,
- $s_x^2$ is the sample variance of driving distance.

## 3. Hierarchical (Multilevel) Linear Regression

In many experimental and observational settings, data are structured in groups — for example, repeated measurements within subjects, students within schools, or brain regions within individuals. In such cases, a single-level regression model may fail to account for the nested structure of the data, leading to incorrect estimates of uncertainty and inflated Type I errors. **Hierarchical** or **multilevel linear regression** extends the basic regression framework by introducing parameters that can vary across groups while being partially pooled through higher-level distributions.

A hierarchical model introduces levels of variation. For example, suppose we have multiple subjects, each with their own regression line relating $x$ and $y$. Instead of fitting independent regressions for each subject, or a single pooled model for all, we assume that the individual regression coefficients $\beta_{0j}$ and $\beta_{1j}$ for subject $j$ come from population-level (group) distributions:

$$
\begin{aligned}
y_{ij} &\sim N(\beta_{0j} + \beta_{1j} x_{ij}, \sigma^2), \\
\beta_{0j} &\sim N(\mu_{\beta_0}, \tau_0^2), \\
\beta_{1j} &\sim N(\mu_{\beta_1}, \tau_1^2).
\end{aligned}
$$

Here, $\mu_{\beta_0}$ and $\mu_{\beta_1}$ represent the **population means** of the intercepts and slopes, while $\tau_0^2$ and $\tau_1^2$ capture the **between-group variability**. The within-group variability is represented by $\sigma^2$. 

This model allows **partial pooling**: each group’s coefficients are estimated based on both the data from that group and the population-level trend. If $\tau^2$ is large, groups are estimated almost independently (little pooling); if $\tau^2$ is small, group estimates shrink strongly toward the overall mean (strong pooling). This property makes hierarchical regression especially powerful when the number of observations per group is small or unequal.

In a Bayesian framework, priors can be placed on both population-level parameters $(\mu_{\beta}, \tau^2)$ and group-level coefficients $(\beta_j)$, resulting in a model that can flexibly represent multiple sources of uncertainty and dependency. Posterior inference provides estimates of both group-specific and population-level effects, as well as credible intervals that correctly reflect the hierarchical structure of the data.

---

## 4. Generalized Linear Models (GLMs)

The standard linear regression model assumes that the response variable $Y$ is continuous, normally distributed, and linearly related to the predictors. However, many types of data violate these assumptions: for example, binary outcomes (success/failure), counts, or positive-only measurements. **Generalized Linear Models (GLMs)** extend linear regression to handle such cases by allowing the response to follow a distribution from the *exponential family* (e.g., binomial, Poisson, gamma) and by introducing a **link function** that connects the expected value of $Y$ to the linear predictor.

A GLM can be written as:

$$
\begin{aligned}
E[Y_i] &= \mu_i, \\
g(\mu_i) &= \eta_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik},
\end{aligned}
$$

where $g(\cdot)$ is the **link function** (for example, the logit link for binary data or the log link for counts), and $\eta_i$ is the **linear predictor**. 

Examples of commonly used GLMs include:

| Model Type | Response Distribution | Link Function | Example Use Case |
|-------------|----------------------|----------------|------------------|
| Linear Regression | Normal | Identity ($g(\mu)=\mu$) | Continuous outcomes |
| Logistic Regression | Binomial | Logit ($\log(\mu/(1-\mu))$) | Binary classification |
| Poisson Regression | Poisson | Log ($\log(\mu)$) | Count data |
| Gamma Regression | Gamma | Inverse or Log | Reaction times, rates |

In the Bayesian setting, GLMs can be easily extended by placing priors on the regression coefficients and hyperparameters, allowing coherent uncertainty quantification for all model parameters. Moreover, hierarchical extensions of GLMs (hierarchical logistic or Poisson regression) can naturally accommodate nested or grouped data structures, combining the flexibility of GLMs with the strength of multilevel modeling.


## 5. Linear Dependence in the Bayesian Framework

In the Bayesian framework, the idea of linear dependence is not restricted to classical regression of the mean of a normally distributed outcome. Rather, **any parameter in the likelihood** — such as the mean, variance, rate, or shape — can be modeled as a linear (or nonlinear) function of predictors. This flexibility allows us to express structured relationships between covariates and parameters of complex probabilistic models.

For example, suppose we have data modeled by an **exponential-Gaussian distribution** (a distribution used for reaction times or other positively skewed data). The likelihood may depend on parameters such as the Gaussian mean $\mu$, standard deviation $\sigma$, and exponential rate $\lambda$. In a Bayesian model, we can define linear predictors for any of these parameters:

$$
\begin{aligned}
\mu_i &= \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}, \\
\log \sigma_i &= \gamma_0 + \gamma_1 z_{i1} + \dots + \gamma_m z_{im}, \\
\log \lambda_i &= \delta_0 + \delta_1 w_{i1} + \dots + \delta_p w_{ip}.
\end{aligned}
$$

Here, each parameter of the likelihood depends on its own set of covariates through a linear relationship. The link functions (e.g., logarithm for $\sigma$ and $\lambda$ to ensure positivity) serve the same role as in generalized linear models.

This approach generalizes regression modeling: instead of specifying a single linear relation for the mean, we define a **model hierarchy** in which covariates can influence multiple aspects of the data-generating process. Bayesian inference then provides joint posterior distributions over all regression coefficients and parameters, automatically propagating uncertainty through the full model.

Such parameterized models are widely used in **hierarchical cognitive models**, **reaction-time modeling**, and **connectivity analysis**, where both the mean level and variability of a process may systematically depend on experimental or individual-level predictors.
