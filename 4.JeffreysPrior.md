# Jeffreys Prior

In Bayesian statistics, choosing a uniform prior is a common way to express ignorance or a lack of prior information about a parameter. However, **uniform priors** are not **invariant to reparameterization**. This means that different parameterizations of the same problem can lead to different priors, and thus different posteriors, even if they seem to be uniformly distributed.

## The Problem with Uniform Priors

Let's consider an example using the **normal distribution**. Suppose we want to specify a prior for the variance $\sigma^2$. There are two natural choices for a uniform prior:

1. A uniform prior on $\sigma^2$:
   $$
   f(\sigma^2) \propto 1
   $$

2. A uniform prior on $\log(\sigma^2)$, which is proportional to $\frac{1}{\sigma^2}$:
   $$
   f(\sigma^2) \propto \frac{1}{\sigma^2}
   $$

Both of these priors are uniform in their respective scales, but they are **different priors** and will lead to different posterior distributions. This shows that uniform priors are not invariant to transformations of the parameter space.

## Jeffreys Prior: Invariant to Reparameterization

To address this issue, **Harold Jeffreys** proposed a solution in the form of the **Jeffreys prior**, which is designed to be **invariant to reparameterization**. The Jeffreys prior is defined as:

$$
f(\theta) \propto \sqrt{I(\theta)}
$$

where $I(\theta)$ is the **Fisher information** for the parameter $\theta$. The Fisher information [[Fisher Information]] measures how much information the data carries about the parameter $\theta$. The Jeffreys prior thus adjusts for changes in the parameterization by accounting for how much information is provided by the data in the new parameterization.

In most cases, the Jeffreys prior is **improper** (i.e., it does not integrate to 1), but it can still be useful for Bayesian inference.

### Jeffreys Prior for Normal Distribution

For the **normal distribution** with unknown mean $\mu$ and variance $\sigma^2$, the Jeffreys prior is:

1. **Uniform** for $\mu$:
   $$
   f(\mu) \propto 1
   $$

2. **Uniform on the log scale** for $\sigma^2$:
   $$
   f(\sigma^2) \propto \frac{1}{\sigma^2}
   $$

This Jeffreys prior is invariant to transformations of the parameters, meaning that regardless of how we reparameterize the normal distribution, we will be using the same prior information.

### Jeffreys Prior for Bernoulli/Binomial Distribution

For a **Bernoulli** or **binomial** distribution with parameter  $\theta$ (the probability of success), the Jeffreys prior is:

$$
f(\theta) \propto \theta^{-\frac{1}{2}} (1 - \theta)^{-\frac{1}{2}}
$$

This is a **Beta distribution** with parameters $\frac{1}{2}$ and $\frac{1}{2}$, which can be written as:

$$
\theta \sim \text{Beta}\left(\frac{1}{2}, \frac{1}{2}\right)
$$

This is one of the few cases where the Jeffreys prior is **proper**. The Jeffreys prior here reflects an effective sample size of **one data point**. Importantly, it is invariant to reparameterization, so it remains the same whether we model the probability $\theta$ directly or use a **logistic transformation**.

## Related Approaches: Reference Priors and Maximum Entropy Priors

Other approaches to **objective Bayesian inference** include:

- **Reference priors**: These priors are designed to maximize the information contributed by the data relative to the prior.
- **Maximum entropy priors**: These are chosen by maximizing entropy, which ensures the least informative prior given any known constraints.

## Empirical Bayes: A Related Concept

A related concept is **empirical Bayes**, where the data is used to help inform the prior. For instance, you might use the mean of the data to set the mean of the prior distribution. While this approach often leads to reasonable point estimates, it has a drawback: it uses the data twice (once in the prior and once in the likelihood), which can lead to **underestimated uncertainty** in the posterior.

## Conclusion

The **Jeffreys prior** is an important tool in Bayesian statistics for creating priors that are **invariant to reparameterization**. It is especially useful when we want to avoid the subjective nature of choosing priors and aim to let the data drive the posterior inference. However, it's crucial to understand that even objective priors like Jeffreys priors may still contain some implicit information, as seen in the case of the **Beta(1/2, 1/2)** prior for Bernoulli distributions.
