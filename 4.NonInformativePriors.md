
# Non-Informative and weakly informative Priors

In Bayesian statistics, we often use **priors** to encode our beliefs or information about a parameter before seeing the data. So far, we've discussed examples of **informative priors**, which contain a significant amount of information. But what if we don't want to put too much information into our prior? This leads us to **non-informative priors**, also referred to as **objective Bayesian inference**.

## What Are Non-Informative Priors?

Non-informative priors are priors designed to **minimize** the information contributed by the prior, allowing the **data** to have the maximum influence on the posterior. In other words, they aim to express a lack of prior knowledge about the parameter values.

### Example: Coin Flipping and the Uniform Prior

Consider a coin-flipping scenario where the data comes from a Bernoulli distribution with an unknown parameter $\theta$ (the probability of heads). If we want to use a **non-informative prior**, one intuitive choice might be to assume that **all values of $\theta$** are equally likely, which can be represented as:

$$
\theta \sim \text{Uniform}(0, 1)
$$

This uniform prior expresses that we have no preference for any value of $\theta$ between 0 and 1. However, note that a **Uniform(0, 1)** prior is equivalent to a **Beta(1, 1)** distribution. The **effective sample size** of a Beta distribution is the sum of its parameters, so in this case, the effective sample size is:

$$
\text{Effective sample size} = 1 + 1 = 2
$$

This is equivalent to having already observed **one head and one tail**. So, even though the prior seems non-informative, it still contains some information.

### Beta Priors with Less Information

If we want a prior with **less information**, we can use a Beta distribution with smaller parameters, such as:

$$
\theta \sim \text{Beta}\left(\frac{1}{2}, \frac{1}{2}\right)
$$

This prior has an effective sample size of 1, which represents less information than a Beta(1, 1). We can reduce the information even further, for instance:

$$
\theta \sim \text{Beta}(0.001, 0.001)
$$

This prior has a tiny effective sample size and exerts very little influence on the posterior, allowing the **data** to dominate the posterior distribution.

### Improper Priors

In the extreme case, we can consider the **limit** of this process, approaching what is called an **improper prior**. For example, consider the limiting case of a **Beta(0, 0)** prior:

$$
\pi(\theta) \propto \theta^{-1} (1 - \theta)^{-1}
$$

This is **not** a proper density because it does not integrate to 1 (its integral is infinite). However, improper priors can still be useful in Bayesian analysis as long as the **posterior** remains proper (i.e., it integrates to 1).

#### Posterior from an Improper Prior

Suppose we observe data from a coin flip with $y$ heads and $n - y$ tails. Using the improper prior above, the posterior distribution for $\theta$ becomes:

$$
\pi(\theta \mid y) \propto \theta^{y - 1} (1 - \theta)^{n - y - 1}
$$

This is a **Beta distribution**:

$$
\theta \mid y \sim \text{Beta}(y, n - y)
$$

Thus, the posterior mean is:

$$
\mathbb{E}[\theta \mid y] = \frac{y}{n}
$$

This matches the **maximum likelihood estimate** (MLE) for $\theta$, showing that the improper prior leads to point estimates similar to the **frequentist approach**, but with the added benefit of a full posterior distribution.

### Non-Informative Priors in the Normal Case

#### Known Variance

Consider the case where the data $Y_i$ are i.i.d. normal with known variance $\sigma^2$ and unknown mean $\mu$. A common non-informative prior for $\mu$ is a **flat prior**, which can be thought of as:

$$
\mu \sim \text{Uniform}(-\infty, \infty)
$$

This prior is improper because it does not integrate to 1 over the real line. However, when we compute the posterior for $\mu$, it is:

$$
\mu \mid y \sim \mathcal{N}\left( \bar{y}, \frac{\sigma^2}{n} \right)
$$

where $\bar{y}$ is the sample mean. This matches the MLE for $\mu$, but again, we get the added benefit of having a posterior distribution.

#### Unknown Variance

For the case of unknown variance $\sigma^2$, a common non-informative prior is:

$$
\pi(\sigma^2) \propto \frac{1}{\sigma^2}
$$

This is equivalent to an **Inverse-Gamma(0, 0)** prior and is improper. The posterior for $\sigma^2$ will be:

$$
\sigma^2 \mid y \sim \text{Inverse-Gamma}\left( \frac{n-1}{2}, \frac{1}{2} \sum (Y_i - \bar{Y})^2 \right)
$$

This is closely related to the frequentist estimate for the sample variance.



## Weakly Informative Priors

A *weakly informative prior* provides light regularization without strongly biasing the results.  
It prevents extreme or implausible estimates that could arise from limited data.  

**Examples:**
- $\beta_i \sim \mathcal{N}(0, 2.5)$ for regression coefficients 
- $\sigma \sim \text{HalfNormal}(1)$ for standard deviations
- $p \sim \text{Beta}(2,2)$ for probabilities centered at 0.5 but flexible

## Key Points

- **Improper priors** are non-informative priors that do not integrate to 1 but can still be used in Bayesian inference as long as the **posterior** is proper.
- Non-informative priors allow the **data** to have maximum influence on the posterior, leading to results that are similar to the frequentist approach in terms of point estimates.
- In some cases, improper priors lead to **posteriors** that are mathematically tractable and match **frequentist estimators**, but they also provide full posterior distributions that allow for credible intervals and probability statements about the parameters.
