# Posterior Predictive Distribution for Coin Flips

## Introduction

After observing data from one or more coin flips, we can use the information to update our beliefs about the probability of heads, $\theta$. This update affects our prediction for future coin flips. Specifically, we want to calculate the *posterior predictive distribution* for the number of heads in future flips, given the data we've already observed.

## Setting Up the Problem

Let's consider the case where we flip a coin once and observe a head. We want to predict the outcome of a second flip. Our predictive distribution now depends on the posterior distribution of $\theta$, given the observed data from the first flip.

## Posterior Predictive Distribution

The posterior predictive distribution for $y_2$, given $y_1$, is defined as:

$$
f(y_2 \mid y_1) = \int_0^1 f(y_2 \mid \theta, y_1) \, f(\theta \mid y_1) \, d\theta
$$

Since $y_1$ and $y_2$ are independent given $\theta$, we can simplify this to:

$$
f(y_2 \mid y_1) = \int_0^1 f(y_2 \mid \theta) \, f(\theta \mid y_1) \, d\theta
$$

### Prior Distribution for $\theta$

Initially, we assume a uniform prior distribution for $\theta$ over $[0, 1]$:

$$
f(\theta) = 1
$$

After observing a head on the first flip ($y_1 = 1$), the posterior distribution for $\theta$ becomes:

$$
f(\theta \mid y_1 = 1) = 2\theta
$$

This is because the posterior distribution, in this case, follows a Beta distribution with parameters $\alpha = 2$ and $\beta = 1$, representing one observed head.

### Predictive Distribution for the Second Flip

To find the posterior predictive distribution for the second flip, we use:

$$
f(y_2 \mid y_1 = 1) = \int_0^1 \theta^{y_2} (1 - \theta)^{1 - y_2} \cdot 2\theta \, d\theta
$$

#### Probability of $y_2 = 1$ (Getting a Head)

For $y_2 = 1$, the expression becomes:

$$
f(y_2 = 1 \mid y_1 = 1) = \int_0^1 2\theta^2 \, d\theta
$$

Calculating this integral:

$$
\int_0^1 2\theta^2 \, d\theta = \left[ \frac{2\theta^3}{3} \right]_0^1 = \frac{2}{3}
$$

Thus, the probability of getting a head on the second flip, given that we observed a head on the first flip, is $\frac{2}{3}$.

#### Probability of $y_2 = 0$ (Getting a Tail)

Similarly, the probability of getting a tail ($y_2 = 0$) is:

$$
f(y_2 = 0 \mid y_1 = 1) = 1 - \frac{2}{3} = \frac{1}{3}
$$

### Interpretation

The posterior predictive distribution reflects a combination of the prior information and the observed data. In this case:

- Before observing any flips, we had a uniform prior over $\theta$, equivalent to assuming we had observed one head and one tail.
- After observing one head, it's as if we've observed two heads and one tail in total.

Therefore, our predictive distribution for the next flip is updated to reflect a probability of $\frac{2}{3}$ for heads and $\frac{1}{3}$ for tails.

## Conclusion

The posterior predictive distribution incorporates both the prior beliefs and the observed data to make predictions about future outcomes. In this example, observing one head makes it more likely to predict a head for the next flip, as our posterior belief in the probability of heads, $\theta$, has increased.

# Bernoulli likelihood example


In Bayesian statistics, when we have a Bernoulli likelihood with $N$ observations (or a binomial likelihood), and we use a beta prior, the posterior distribution can be expressed as:

$$
\text{Posterior} \sim \text{Beta}(\alpha + \sum y_i, \beta + n - \sum y_i)
$$

Here, it is clear how both the prior and the data contribute to the posterior. A beta prior $\text{Beta}(\alpha, \beta)$ is equivalent to having $\alpha + \beta$ additional observations, which we can refer to as the **effective sample size of the prior**.

## 2. Posterior Mean Calculation

The expected value (mean) of a beta distribution is given by the first parameter over the sum of the parameters:

$$
\text{Posterior Mean} = \frac{\alpha + \sum y_i}{\alpha + \sum y_i + \beta + n - \sum y_i}
$$

This can be simplified as:

$$
\text{Posterior Mean} = \frac{\alpha + \sum y_i}{\alpha + \beta + n}
$$

We can further decompose this into:

$$
\frac{\alpha + \beta}{\alpha + \beta + n} \cdot \frac{\alpha}{\alpha + \beta} + \frac{n}{\alpha + \beta + n} \cdot \frac{\sum y_i}{n}
$$

What we see here is that the posterior mean is a weighted average of the prior mean and the data mean.

- **Prior Mean**: $\frac{\alpha}{\alpha + \beta}$
- **Data Mean**: $\frac{\sum y_i}{n}$

The weights for the prior and the data add up to one, demonstrating their influence on the posterior mean.

## 3. Influence of the Prior

The effective sample size of the prior $\alpha + \beta$ gives insight into how much data is needed to ensure the prior doesn't overly influence the posterior:

- If $\alpha + \beta$ is small compared to $N$, the posterior will be predominantly driven by the data.
- If $\alpha + \beta$ is large relative to $N$, the prior will have a significant impact on the posterior.

## 4. Credible Intervals vs. Confidence Intervals

In frequentist statistics, a 95% confidence interval for $\theta$ is computed as:

$$
\hat{\theta} \pm 1.96 \sqrt{\frac{\hat{\theta}(1 - \hat{\theta})}{n}}
$$

In contrast, a Bayesian credible interval can be derived from the posterior distribution. Using a computer package, we can numerically find an interval that has a 95% probability of containing $\theta$. This is a true probability statement, unlike the frequentist approach.

## 5. Sequential Updates in Bayesian Statistics

Bayesian statistics allows for straightforward sequential updates:

1. **Start with Prior**: Begin with the prior distribution $f(\theta)$.
2. **Observe Data**: After observing $n$ data points $y_1, \ldots, y_n$, update the prior to obtain the posterior $f(\theta \mid y_1, \ldots, y_n)$.
3. **New Data**: If you observe more data points $y_{n+1}, \ldots, y_{n+m}$, treat the previous posterior as the new prior and update it with the new data.

This can be expressed as:

$$
f(\theta \mid y_1, \ldots, y_{n+m}) = f(\theta \mid y_1, \ldots, y_n) \cdot f(y_{n+1}, \ldots, y_{n+m} \mid \theta)
$$

This sequential updating yields the same posterior as if all data were processed at once.

## 6. Applications in Medical Device Testing

One area where Bayesian statistics has been particularly useful is in **medical device testing**. Given the often small sample sizes and iterative testing processes, the ability to perform sequential updates makes Bayesian methods appealing for regulatory purposes. 

The combination of prior knowledge and observed data allows researchers to make informed decisions while minimizing risks associated with new devices.

## Conclusion

In summary, Bayesian inference provides a robust framework for updating beliefs about parameters based on observed data, leveraging prior distributions effectively. The ability to perform sequential updates further enhances its practical applications, particularly in fields such as medical device testing.
