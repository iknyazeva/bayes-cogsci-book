# Frequentist Approach to Inference

## Frequentist Paradigm

In the frequentist paradigm, data is viewed as a random sample from a larger, potentially hypothetical population. Probability statements are made in terms of long-run frequencies based on this larger population.

### Example: Coin Flips

Suppose we flip a coin 100 times and observe 44 heads and 56 tails. We can consider these 100 flips as a random sample from a much larger, infinite hypothetical population of flips from this coin. In this case, each flip $x_i$ follows a Bernoulli distribution with some probability $p$. Although $p$ may be unknown, it is assumed to be fixed because we have a particular physical coin.

### Estimating the Probability of Heads

We can estimate the probability of getting a head ($p$) and assess our confidence in that estimate.

Using the Central Limit Theorem (CLT), we can approximate the sum of the 100 flips ($\sum x_i$) as normally distributed:

$$
\sum x_i \sim \mathcal{N}(100p, 100p(1-p))
$$

According to the CLT, 95% of the time, our observations will fall within 1.96 standard deviations of the mean. This can be expressed as:

$$
100p - 1.96 \sqrt{100p(1-p)} \leq \sum x_i \leq 100p + 1.96 \sqrt{100p(1-p)}
$$

Given our observation of 44 heads, we estimate $\hat{p}$ as:

$$
\hat{p} = \frac{44}{100} = 0.44
$$

### Confidence Interval Calculation

Plugging this estimate into the theoretical bounds, we calculate the Confidence Interval (CI) for $p$:

$$
44 \pm 1.96 \sqrt{44 \times 0.56} = 44 \pm 9.7
$$

Thus, the CI ranges from approximately 34.3 to 53.7. Converting this to a probability statement:

$$
0.343 \leq p \leq 0.537
$$

We are 95% confident that the true probability of getting a head lies within this interval.

### Interpretation of the Confidence Interval

If we want to assess whether the coin is fair (i.e., $p = 0.5$), we see that 0.5 falls within our interval, suggesting that it is reasonable to assume fairness.

However, interpreting what "95% confidence" means under the frequentist paradigm requires understanding the concept of repeated sampling. It means that if we were to repeat this trial many times and calculate a CI each time, about 95% of those intervals would contain the true value of $p$. For any given interval, the probability that $p$ lies within it is technically 0 or 1 â€” it either does or does not.

## Contrast with Bayesian Approach

In the Bayesian approach, we can directly compute an interval and say that there is a 95% probability that  $p$ lies within that interval, based on the posterior distribution.

### Frequentist vs. Bayesian Intervals

- **Frequentist Confidence Interval**: Interpreted as the long-run frequency of intervals that contain the true parameter value if the experiment were repeated many times.
- **Bayesian Credible Interval**: Directly interpreted as the probability that the parameter lies within the interval given the observed data.

### Choosing Between Approaches

- **Frequentist CI**: Preferred when you want to make statements about the long-run frequency of an event occurring based on repeated sampling.
- **Bayesian Credible Interval**: Preferred when you want to make probability statements about the parameter given the observed data, incorporating prior knowledge.

Choosing between these approaches often depends on the context and the type of inference desired. The Bayesian approach provides a more intuitive interpretation in many cases but requires a prior, which might not always be available or easily defined.



## Example: estimating Mortality Rate Using the Frequentist Approach

In this tutorial, we'll explore the frequentist approach to estimating the mortality rate in a hospital setting. Suppose 400 patients are admitted to a hospital over a month for heart attacks. A month later, 72 of them have died, and 328 have survived. We aim to estimate the mortality rate using the frequentist approach.

### Establishing the Reference Population

Under the frequentist paradigm, we need to establish a reference population. Possible choices include:

1. **Heart attack patients in the region.**
2. **Heart attack patients admitted to this hospital over a longer period of time.**

While both options are reasonable, our data is not a random sample from either population. A more hypothetical reference could be all people in the region who might have a heart attack and be admitted to this hospital. However, this setup presents philosophical challenges within the frequentist paradigm.

## Estimation Approach

### Bernoulli Distribution

We can model each patient's outcome (survival or death) using a Bernoulli distribution with an unknown parameter $\theta$. Here, $\theta$ represents the probability of mortality (i.e., the patient dies). For each individual $i$:

$$
P(Y_i = 1) = \theta
$$

### Probability Density Function

The probability density function for the entire set of data (denoted as a vector $\mathbf{Y}$) given $\theta$ is:

$$
P(\mathbf{Y} = \mathbf{y} \mid \theta) = \prod_{i=1}^{n} P(Y_i = y_i \mid \theta) = \prod_{i=1}^{n} \theta^{y_i} (1 - \theta)^{1 - y_i}
$$

This product form arises because each observation is independent.

### Likelihood Function

Now, consider this expression as a function of $\theta$. This is known as the **likelihood function**:

$$
L(\theta \mid \mathbf{y}) = \prod_{i=1}^{n} \theta^{y_i} (1 - \theta)^{1 - y_i}
$$

Although it resembles the probability density, it is now viewed as a function of $\theta$ given the data $\mathbf{y}$. 

### Maximum Likelihood Estimate (MLE)

To estimate $\theta$, we choose the value that maximizes the likelihood function, known as the **Maximum Likelihood Estimate (MLE)**. In practice, it's easier to maximize the **log-likelihood**:

$$
\log L(\theta) = \sum_{i=1}^{n} \left( y_i \log \theta + (1 - y_i) \log (1 - \theta) \right)
$$

The logarithm simplifies the product into a sum, making it easier to handle mathematically.

### Simplifying the Log-Likelihood

For independent and identically distributed (i.i.d) data:

$$
\log L(\theta) = \left( \sum_{i=1}^{n} y_i \right) \log \theta + \left( \sum_{i=1}^{n} (1 - y_i) \right) \log (1 - \theta)
$$

Substituting the sums:

$$
\log L(\theta) = S \log \theta + (n - S) \log (1 - \theta)
$$

where $S = \sum_{i=1}^{n} y_i$ is the total number of successes (deaths in this context).

### Maximizing the Log-Likelihood

To find the MLE of $\theta$, we take the derivative of the log-likelihood with respect to $\theta$ and set it to zero:

$$
\frac{d}{d\theta} \log L(\theta) = \frac{S}{\theta} - \frac{n - S}{1 - \theta} = 0
$$

Simplifying:

$$
\frac{S}{\theta} = \frac{n - S}{1 - \theta}
$$

Solving for $\theta$:

$$
\hat{\theta} = \frac{S}{n}
$$

### Applying the Formula

In our example:

- $n = 400$ (total patients)
- $S = 72$ (patients who died)

Thus:

$$
\hat{\theta} = \frac{72}{400} = 0.18
$$

This means the MLE for the mortality rate is 0.18, or 18%.

In Python code for likelihood function

```python
def likelihood(n, y, theta):
    lik = theta**y*(1-theta)**(n-y)
    return lik
theta = np.linspace(0,1,100)
plt.plot(theta, likelihood(400, 72, theta));
plt.title('Likelihood function for mortality rate,\n when 72 out of 400 died');
plt.tight_layout()
```

![[LikeMort.png]]

## Conclusion

The MLE provides a straightforward method for estimating parameters under the frequentist approach, by maximizing the likelihood function based on the observed data. While setting up reference populations and interpreting results can be challenging, especially with hypothetical scenarios, the frequentist method remains a foundational approach in statistical inference.

