# Basics of Monte Carlo Estimation


In Bayesian inference, our goal is often to characterize the posterior distribution $p(\theta \mid y)$, where $\theta$ represents model parameters and $y$ is the observed data. While we can compute this posterior analytically for simple models (e.g., using conjugate priors), real-world problems involve complex likelihoods and priors, making the normalizing constant $p(y)$ intractable.

Monte Carlo estimation involves simulating hypothetical draws from a probability distribution to estimate important properties like mean, variance, probabilities, or quantiles. It is a powerful method, particularly useful when analytical integration is challenging or impossible.

## Key Concepts of Monte Carlo Estimation

### The Basic Idea
Monte Carlo estimation uses **random sampling** to approximate integrals that represent quantities of interest, such as:
- The mean
- The variance
- The probability of an event
- Quantiles of the distribution

## Example 1: Estimating the Mean of a Gamma Distribution

### Problem Setup

Suppose $\theta$ follows a Gamma distribution:
$$
\theta \sim \text{Gamma}(a, b)
$$
where:
- $a = 2$ (shape parameter)
- $b = \frac{1}{3}$ (rate parameter)

The mean of $\theta$ is given by:
$$
\mathbb{E}[\theta] = \int_{0}^{\infty} \theta \cdot f(\theta) \, d\theta
$$
For a Gamma distribution, this integral simplifies to:
$$
\mathbb{E}[\theta] = \frac{a}{b} = 6
$$

### Verifying with Monte Carlo Estimation

To estimate the mean:
1. Simulate $m$ random draws, $\theta^*_i$, from $\text{Gamma}(2, \frac{1}{3})$.
2. Compute the **sample mean**:
$$
\bar{\theta}^* = \frac{1}{m} \sum_{i=1}^{m} \theta^*_i
$$

By the **Law of Large Numbers**:
- As $m \to \infty$, $\bar{\theta}^* \to \mathbb{E}[\theta]$.

By the **Central Limit Theorem**:
- $\bar{\theta}^*$ approximately follows:
$$
\mathcal{N}\left(\mathbb{E}[\theta], \frac{\text{Var}(\theta)}{m}\right)
$$

---

### Estimating Variance

The variance of $\theta$ is:
$$
\text{Var}(\theta) = \mathbb{E}[\theta^2] - (\mathbb{E}[\theta])^2
$$

Monte Carlo can estimate this by:
1. Simulating $m$ draws $\theta^*_i$.
2. Computing the **sample variance**:
$$
\widehat{\text{Var}}(\theta) = \frac{1}{m} \sum_{i=1}^{m} (\theta^*_i - \bar{\theta}^*)^2
$$


## Estimating Probabilities

### Indicator Functions for Probabilities

Suppose $h(\theta)$ is an **indicator function**:
$$
h(\theta) = \begin{cases} 
1 & \text{if } \theta < 5 \\
0 & \text{otherwise}
\end{cases}
$$

The expected value of $h(\theta)$ is:
$$
\mathbb{E}[h(\theta)] = P(\theta < 5)
$$

### Monte Carlo Approach
1. Simulate $m$ random draws $\theta^*_i$.
2. Count how many satisfy $\theta^*_i < 5$:
$$
P(\theta < 5) \approx \frac{1}{m} \sum_{i=1}^{m} h(\theta^*_i)
$$

---

## Example 3: Estimating Quantiles

Quantiles represent thresholds below which a certain percentage of the data lies. For instance, the **90th percentile** is the value $z$ such that:
$$
P(\theta < z) = 0.9
$$

### Monte Carlo Approach
1. Simulate $m$ random draws $\theta^*_i$.
2. Sort $\theta^*_i$ in ascending order.
3. Select the value at the 90th percentile of the sorted list.



# Evaluating Monte Carlo Approximations 

Monte Carlo methods are powerful tools, but how reliable are their approximations? We explore how the **Central Limit Theorem** (CLT) helps assess the accuracy of Monte Carlo estimates and how to extend Monte Carlo sampling to hierarchical models.

---

## 1. Assessing the Accuracy of Monte Carlo Approximations

### Central Limit Theorem and Variance of Estimates

The **CLT** tells us that for a Monte Carlo estimate, the sample mean $\bar{\theta}^*$ approximately follows a normal distribution:
$$
\bar{\theta}^* \sim \mathcal{N}\left(\mathbb{E}[\theta], \frac{\text{Var}(\theta)}{m}\right)
$$
where:
- $\mathbb{E}[\theta]$: True expected value of $\theta$
- $\text{Var}(\theta)$: True variance of $\theta$
- $m$: Number of samples

The variance of the sample mean decreases with larger $m$, improving the approximation.

---

### Estimating Variance and Standard Error

The **sample variance** provides an approximation of the true variance:
$$
\widehat{\text{Var}}(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left(\theta^*_i - \bar{\theta}^*\right)^2
$$

The **standard error** of the Monte Carlo estimate is:
$$
\text{SE}(\bar{\theta}^*) = \sqrt{\frac{\widehat{\text{Var}}(\theta)}{m}}
$$

Using this standard error, we can quantify the uncertainty in our estimate. For large $m$, the true value is likely within approximately $\pm 2 \cdot \text{SE}(\bar{\theta}^*)$ of the Monte Carlo estimate.

---

## Monte Carlo Sampling in Hierarchical Models

### Example: Binomial Hierarchical Model

Consider a **hierarchical model** where:
- $y \mid \phi \sim \text{Binomial}(n=10, \phi)$
- $\phi \sim \text{Beta}(\alpha=2, \beta=2)$

This structure models $y$ as a binomial random variable with a random success probability $\phi$, which itself follows a Beta prior.

---

### Joint Distribution and Simulation Steps

The **joint distribution** of $y$ and $\phi$ is:
$$
p(y, \phi) = p(\phi) \cdot p(y \mid \phi)
$$
Here, $p(\phi)$ is the prior (Beta distribution), and $p(y \mid \phi)$ is the likelihood (Binomial distribution).

To simulate from this joint distribution:
1. **Sample $\phi^*_i$:** Draw $\phi^*_i \sim \text{Beta}(2, 2)$.
2. **Sample $y^*_i$:** Given $\phi^*_i$, draw $y^*_i \sim \text{Binomial}(10, \phi^*_i)$.

Repeat these steps $m$ times to generate $m$ independent pairs $\{(y^*_i, \phi^*_i)\}$.

---

### Marginal Distributions and Prior Predictive Distributions

One advantage of Monte Carlo sampling is the ability to approximate marginal distributions easily:
- The marginal distribution of $y$ can be obtained by discarding $\phi^*_i$ values and using $y^*_i$ as samples.

This technique also produces **prior predictive distributions**, which describe the variability in $y$ based on the prior assumptions about $\phi$.

---

## Key Takeaways

- **Accuracy of Monte Carlo estimates** depends on the sample size $m$, with variance decreasing as $m$ increases.
- The **standard error** quantifies the uncertainty in a Monte Carlo estimate, and the true value is likely within $\pm 2 \cdot \text{SE}$ of the estimate for large $m$.
- Monte Carlo simulation simplifies working with **hierarchical models** by enabling direct sampling from joint distributions and easy marginalization.

In the next lesson, we'll learn how to sample from **complex posterior distributions**, combining these principles to perform approximate Bayesian inference.

---

---

## Advantages of Monte Carlo Estimation
- **Flexibility**: Works for complex distributions where analytical solutions are intractable.
- **Versatility**: Can estimate means, variances, probabilities, and quantiles.

Monte Carlo methods form the backbone of many statistical and computational tools, enabling practical solutions for complex problems.
