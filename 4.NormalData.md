# Bayesian Inference with the Normal Distribution


## Normal Distribution with Known Variance, Unknown Mean
In this example, we'll assume that the **variance** (or standard deviation) is known, and we're only interested in learning about the **mean** of the normal distribution. This scenario frequently occurs in industrial monitoring, where processes are tracked for consistency.

### Normal Distribution Likelihood
If the data $Y$ is normally distributed with a known variance $\sigma^2$ and an unknown mean $\mu$, the likelihood function is:

$$
Y \sim \mathcal{N}(\mu, \sigma^2)
$$

The likelihood of observing $Y$ given $\mu$ is:

$$
L(\mu \mid Y) \propto \exp\left(-\frac{(Y - \mu)^2}{2\sigma^2}\right)
$$

### Conjugate Prior for the Normal Distribution
When dealing with the **normal likelihood** and unknown mean, a **normal prior** is conjugate. That is, if the prior distribution for $\mu$ is normally distributed, the posterior will also be normally distributed.

Suppose we specify a normal prior for $\mu$ :

$$
\mu \sim \mathcal{N}(\mu_0, \tau_0^2)
$$

where $\mu_0$ is the prior mean and $\tau_0^2$ is the prior variance.

#### Posterior Distribution
When we observe data $Y$, we can update our belief about $\mu$ by combining the prior and likelihood. The posterior distribution for $\mu$ will also be normally distributed with updated parameters:

- **Posterior mean**: The posterior mean is a weighted average of the prior mean and the observed data mean. 

$$ 
\mu_{\text{posterior}} = \frac{1/\tau_0^2}{1/\tau_0^2 + n/\sigma^2} \mu_0 + \frac{n/\sigma^2}{1/\tau_0^2 + n/\sigma^2} \bar{Y} 
$$

where $\bar{Y}$ is the sample mean of the data and $n$ is the sample size.

- **Posterior variance**: The posterior variance shrinks as more data is observed:
  
$$
\tau_{\text{posterior}}^2 = \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} 
$$

### Effective Sample Size for Normal Distribution with Known Variance

The **effective sample size** for a normal distribution with **known variance** quantifies the amount of information in the prior relative to the observed data. It represents how much weight the prior carries when updating our belief about the mean $\mu$.

 **Bayesian Framework**

In a Bayesian framework, we assume a **normal prior** for $\mu$ with mean $\mu_0$and variance $\tau_0^2$:

$$
\mu \sim \mathcal{N}(\mu_0, \tau_0^2)
$$

The **likelihood** of the observed data $X_1, X_2, \dots, X_n$, which are normally distributed with **known variance** $\sigma^2$, is:

$$
X_i \sim \mathcal{N}(\mu, \sigma^2)
$$

Given the prior and likelihood, the **posterior distribution** for $\mu$is normally distributed with an updated mean and variance.

### Effective Sample Size Calculation

The **posterior mean** is a weighted average of the prior mean $\mu_0$and the sample mean $\bar{X}$:

$$
\mu_{\text{posterior}} = \frac{\frac{n}{\sigma^2} \bar{X} + \frac{1}{\tau_0^2} \mu_0}{\frac{n}{\sigma^2} + \frac{1}{\tau_0^2}}
$$

This can be rewritten as:

$$
\mu_{\text{posterior}} = \frac{n \bar{X} + W \mu_0}{n + W}
$$

where $W$ is the **effective sample size** of the prior, defined as:

$$
W = \frac{\sigma^2}{\tau^2}
$$

### Interpretation of $W$

- $W$ reflects the relative influence of the **prior** compared to the **observed data**.
- A **larger $W$** means the prior carries more weight, behaving as though it represents $W$ additional observations.
- A **smaller $W$** means the data carries more influence over the posterior estimate.

### Example

If $\sigma^2 = 4$ and $\tau^2 = 1$, then:

$$
W = \frac{4}{1} = 4
$$

This means that the prior is equivalent to having 4 additional observations. If the observed data size $n = 10$, the total "effective" sample size is $10 + 4 = 14$.

Thus, the **effective sample size** provides a way to quantify the weight of the prior in relation to the actual data sample size.


### Example Calculation
Let’s consider an example where:

- The prior mean is $\mu_0 = 50$
- The prior variance is $\tau_0^2 = 25$
- The known variance of the data is $\sigma^2 = 16$
- We collect a sample of $n = 10$ observations with a sample mean $\bar{Y} = 52$

#### Updating the Prior with Data
Using the formulas for the posterior mean and variance, we calculate:

- **Posterior mean**:

$$ 
\mu_{\text{posterior}} = \frac{25}{25 + \frac{16}{10}} \cdot 50 + \frac{\frac{16}{10}}{25 + \frac{16}{10}} \cdot 52 
$$

Simplifying, we get: $\mu_{\text{posterior}} \approx 50.76$

- **Posterior variance**:

$$ 
\tau_{\text{posterior}}^2 = \frac{1}{\frac{1}{25} + \frac{10}{16}} \approx 10.71 
$$

Thus, the posterior distribution for $\mu$ is:

$$
\mu \mid Y \sim \mathcal{N}(50.76, 10.71)
$$

### Predictive distribution for new data

Consider again the model $x \mid \mu \sim N(\mu, \sigma_0^2)$, where $\mu \sim N(m_0, s_0^2)$ and $\sigma_0^2$ is known. Here we derive the marginal distribution for the data, given by:

$$
\int_{-\infty}^{\infty} f(x \mid \mu) f(\mu) d\mu
$$

This represents the prior predictive distribution for a new data point $x$.

#### Step 1: Rewriting the model

We rewrite the model in a more convenient form:

$$
x = \mu + \epsilon
$$

where $\epsilon \sim N(0, \sigma_0^2)$, and

$$
\mu = m_0 + \eta
$$

where $\eta \sim N(0, s_0^2)$, with $\epsilon$ and $\eta$ independent.

#### Step 2: Substituting $\mu$ into the equation for $x$

Substitute $\mu = m_0 + \eta$ into the equation for $x$:

$$
x = m_0 + \eta + \epsilon
$$

#### Step 3: Finding the distribution of $x$

Recall that adding two normal random variables results in another normal random variable. So, $x$ is normal, and we can compute the mean and variance.

The expected value of $x$ is:

$$
E(x) = E(m_0 + \eta + \epsilon) = m_0 + E(\eta) + E(\epsilon) = m_0 + 0 + 0 = m_0
$$

The variance of $x$ is:

$$
\text{Var}(x) = \text{Var}(m_0 + \eta + \epsilon) = \text{Var}(m_0) + \text{Var}(\eta) + \text{Var}(\epsilon)
$$

Since $\text{Var}(m_0) = 0$, we have:

$$
\text{Var}(x) = s_0^2 + \sigma_0^2
$$

Thus, the marginal distribution for $x$ is:

$$
x \sim N(m_0, s_0^2 + \sigma_0^2)
$$

#### Posterior Predictive Distribution

The posterior predictive distribution follows the same form, but with $m_0$ and $s_0^2$ replaced by their posterior updates based on the observed data.


## Bayesian Inference with Unknown Mean and Variance

When both the mean $\mu$ and variance $\sigma^2$ are **unknown**, we can specify a **conjugate prior** in a hierarchical fashion.

### Hierarchical Model for Normal Data
Suppose we have data $X_i$ that follows an independent, identically distributed (iid) normal distribution with **unknown** mean $\mu$ and variance $\sigma^2$:

$$
X_i \mid \mu, \sigma^2 \sim \mathcal{N}(\mu, \sigma^2)
$$

### Prior for $\mu$ and $\sigma^2$
We can specify the prior in two steps:

1. **Prior for $\mu$ given $\sigma^2$**:
   - $\mu$ is normally distributed, conditional on $\sigma^2$, with mean $M$ and variance $\frac{\sigma^2}{W}$.
   - $W$ represents the **effective sample size** of the prior and scales with $\sigma^2$:

$$ 
\mu \mid \sigma^2 \sim \mathcal{N}\left(M, \frac{\sigma^2}{W}\right)
$$

2. **Prior for $\sigma^2$**:
   - The conjugate prior for $\sigma^2$ is an **[inverse gamma distribution](4.InverseGammaDistribution.md)** with parameters $\alpha$ and $\beta$:

 $$ 
\sigma^2 \sim \text{Inverse-Gamma}(\alpha, \beta)
$$

### Posterior Distributions
After observing data $X = (X_1, \dots, X_n)$, we can compute the **posterior distributions** for both $\mu$ and $\sigma^2$ based on these priors.

1. **Posterior for $\sigma^2$ given $X$**:
   - The posterior for $\sigma^2$ follows an **inverse gamma distribution** with updated parameters:

$$ 
\sigma^2 \mid X \sim \text{Inverse-Gamma}\left(\alpha + \frac{N}{2}, \beta + \frac{1}{2} \left(\sum_{i=1}^N (X_i - \bar{X})^2 + \frac{N W}{N + W} (\bar{X} - M)^2\right)\right)
$$

2. **Posterior for $\mu$ given $\sigma^2$**:
   - The posterior for $\mu$, conditional on $\sigma^2$, is normally distributed:
$$ 
\mu \mid \sigma^2, X \sim \mathcal{N}\left(\frac{N \bar{X} + W M}{N + W}, \frac{\sigma^2}{N + W}\right)
$$

   - Here, $\bar{X}$ is the sample mean of the data, and the posterior mean can be written as a **weighted average** of the prior mean $M$ and the data mean $\bar{X}$.

### Marginalizing Over $\sigma^2$
In many cases, we're primarily interested in inference for $\mu$ and may not care about $\sigma^2$. We can **marginalize out** $\sigma^2$, integrating over its posterior distribution. This results in the marginal posterior for $\mu$, which follows a **Student’s T-distribution**.

- **Marginal posterior for $\mu$**:
  
$$ 
\mu \mid X \sim t\left(\text{loc} = \frac{N \bar{X} + W M}{N + W}, \text{scale} = \sqrt{\frac{\beta + \frac{1}{2} \left(\sum_{i=1}^N (X_i - \bar{X})^2 + \frac{N W}{N + W} (\bar{X} - M)^2\right)}{(N + W)(\alpha + \frac{N}{2})}}, \text{df} = 2\alpha + N \right)
$$

### Posterior Predictive Distribution
The **posterior predictive distribution**, which gives the distribution of future observations based on the data, also follows a **T-distribution**.

### Extensions
This hierarchical model can be extended in several ways:

1. **Multivariate normal case**: This requires vector and matrix notation to model multiple dimensions.
2. **Hierarchical priors**: We can specify hierarchical priors for parameters such as $M$, $W$, and $\beta$ for more complex modeling scenarios.

## Key Takeaways

- The **normal distribution** is common in many real-world applications.
- If the **variance** is known and only the **mean** is unknown, the **normal distribution** is a conjugate prior.
- The posterior mean is a weighted average of the prior mean and the sample mean, with the posterior variance shrinking as more data is observed.
- **Hierarchical priors** allow us to specify a flexible model for both unknown $\mu$ and $\sigma^2$.
- The posterior for $\mu$, conditional on $\sigma^2$, is a **normal distribution**, while the posterior for $\sigma^2$ is an **inverse gamma**.
- **Marginalizing** over $\sigma^2$ results in a **T-distribution** for $\mu$, which is useful when we're primarily interested in the mean.




In this example, we updated our prior belief about $\mu$ using new data, resulting in a new posterior distribution.
