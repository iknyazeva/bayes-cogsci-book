# Discrete model quize

## Q1. Calibrating a Thermometer

Suppose you are trying to calibrate a thermometer by testing the temperature it reads when water begins to boil. Due to natural variation, you take several measurements (experiments) to estimate $\theta$, the mean temperature reading for this thermometer at the boiling point.

You know that at sea level, water should boil at 100 degrees Celsius, so you use a precise prior with $P(\theta = 100) = 1$. You then observe the following five measurements: $94.6, 95.4, 96.2, 94.9, 95.9$

What will the posterior for $\theta$ look like?


1) Most posterior probability will be concentrated near the sample mean of 95.4 degrees Celsius.
2) Most posterior probability will be spread between the sample mean of 95.4 degrees Celsius and the prior mean of 100 degrees Celsius.
3) The posterior will be $\theta = 100$ with probability 1, regardless of the data [+].
4) None of the above. 

  
## Q2. Gamma function

Recall that for positive integer $n$, the gamma function has the following property: $\Gamma(n)=(n−1)!$.
What is the value of $\Gamma(6)$? 


## Q3. Gamma function
Find the value of the normalizing constant $c$ for the integral 

$$
\int_0^1 c \cdot z^3 (1 - z)^1 \, dz
$$

1) $\frac{\Gamma(4+2)}{\Gamma(4)\Gamma(2} = 20$
2) $\frac{\Gamma(3+1)}{\Gamma(3)\Gamma(1} = 3$
3)  $\frac{\Gamma(1)}{\Gamma(z)\Gamma(1-z}$
4) None of the above


## Q4. Posterior predictive
Consider the coin-flipping example. The likelihood for each coin flip was Bernoulli with probability of heads $\theta$, or 

$$
f(y \mid \theta) = \theta^y (1 - \theta)^{1 - y}
$$ 

for $y = 0$ or $y = 1$, and we used a uniform prior on $\theta$.

Recall that if we had observed $Y_1 = 0$ instead of $Y_1 = 1$, the posterior distribution for $\theta$ would have been 

$$
f(\theta \mid Y_1 = 0) = 2(1 - \theta) I _{\{ 0 \leq \theta \leq 1 \}}.
$$ 

Which of the following is the correct expression for the posterior predictive distribution for the next flip $Y_2 \mid Y_1 = 0$?

1)
$$
f(y_2 \mid Y_1 = 0) = \int_0^1 2 \theta^{y_2} (1 - \theta)^{1 - y_2} d\theta \quad \text{for } y_2 = 0 \text{ or } y_2 = 1.
$$

2)
$$
f(y_2 \mid Y_1 = 0) = \int_0^1 \theta^{y_2} (1 - \theta)^{1 - y_2} 2(1 - \theta) d\theta \quad \text{for } y_2 = 0 \text{ or } y_2 = 1.
$$

3) 
$$
f(y_2 \mid Y_1 = 0) = \int_0^1 \theta^{y_2} (1 - \theta)^{1 - y_2} d\theta \quad \text{for } y_2 = 0 \text{ or } y_2 = 1.
$$

4) 
$$
f(y_2 \mid Y_1 = 0) = \int_0^1 2(1 - \theta) d\theta \quad \text{for } y_2 = 0 \text{ or } y_2 = 1.
$$

## Q5 . Prior predictive
The prior predictive distribution for $X$ when $\theta$ is continuous is given by 
$\int f(x \mid \theta) \cdot f(\theta) d\theta$  The analogous expression when $\theta$ is discrete is  $\sum_{\theta} f(x \mid \theta) \cdot f(\theta),$ 
adding over all possible values of $\theta$.

Let's return to the example of your brother's loaded coin. Recall that he has a fair coin where heads comes up on average 50% of the time ($p = 0.5$) and a loaded coin ($p = 0.7$). If we flip the coin five times, the likelihood is binomial: 

$$
f(x \mid p) = \binom{5}{x} p^x (1 - p)^{5 - x},
$$ 

where $X$ counts the number of heads.

Suppose you are confident, but not sure that he has brought you the loaded coin, so that your prior is 

$$
f(p) = 0.9 I \{ p = 0.7 \} + 0.1 I \{ p = 0.5 \}.
$$ 

Which of the following expressions gives the prior predictive distribution of $X$?

1)
$$
f(x) = \binom{5}{x} (0.7)^x (0.3)^{5 - x} (0.9) + \binom{5}{x} (0.5)^x (0.5)^{5 - x} (0.1)
$$

2)
$$
f(x) = \binom{5}{x} (0.7)^x (0.3)^{5 - x} (0.1) + \binom{5}{x} (0.5)^x (0.5)^{5 - x} (0.9)
$$

3)
$$
f(x) = \binom{5}{x} (0.7)^x (0.3)^{5 - x} + \binom{5}{x} (0.5)^x (0.5)^{5 - x}
$$

4)
$$
f(x) = \binom{5}{x} (0.7)^x (0.3)^{5 - x} (0.5) + \binom{5}{x} (0.5)^x (0.5)^{5 - x} (0.5)
$$



## Q6. Posterior distribution

Suppose we use a Bernoulli likelihood for each coin flip, i.e.,

$$
f(y_i \mid \theta) = \theta^{y_i}(1 - \theta)^{1 - y_i} I\{0 \leq \theta \leq 1\}
$$

for $y_i = 0$ or  $y_i = 1$, and a uniform prior for $\theta$. What is the posterior distribution for $\theta$ if we observe the following sequence: (T, T, T, T) where H denotes heads $Y = 1$ and T denotes tails $Y = 0$?
1) $\text{Beta}(1, 5)$
2) $\text{Beta}(1, 4)$
3) $\text{Beta}(0, 4)$
4) $\text{Beta}(4, 0)$
5) $\text{Uniform}(0, 4)$


## Q7. Posterior distribution 
Draw posterior PDF of  $\theta$  if we observe the sequence (T, T, T, T)?


## Q8. MLE

What is the maximum likelihood estimate (MLE) of  $\theta$  if we observe the sequence (T, T, T, T)? 


##  Q9. Posterior mean
What is the posterior mean estimate of $\theta$  if we observe the sequence (T, T, T, T)? Write full solution.



## Q10. Posterior 

Find the posterior probability that $\theta <0.5$  if we observe the sequence (T,T,T,T)


## Q11. Beta-binomial

An engineer wants to assess the reliability of a new chemical refinement process by measuring $\theta$, the proportion of samples that fail a battery of tests. These tests are expensive, and the budget only allows 20 tests on randomly selected samples. Assuming each test is independent, she assigns a binomial likelihood where $X$ counts the samples which fail. Historically, new processes pass about half of the time, so she assigns a $\text{Beta}(2, 2)$ prior for $\theta$ (prior mean 0.5 and prior sample size 4). The outcome of the tests is 6 fails and 14 passes. What is the posterior distribution for $\theta$

1) $\text{Beta}(16, 8)$
2)  $\text{Beta}(6, 14)$
3)  $\text{Beta}(6, 20)$
4)  $\text{Beta}(8, 16)$
5)  $\text{Beta}(14, 6)$


## Q12. Beta binomial
For the previous task, calculate the upper end of an equal-tailed 95% credible interval for  $\theta$


## Q13.  Beta binomial

The engineer tells you that the process is considered promising and can proceed to another phase of testing if we are $90\%$ sure that the failure rate is less than .35.
 Calculate the posterior probability $P(θ<.35∣x)$. In your role as the statistician, would you say that this new chemical should pass?


## Q14.  Beta binomial
It is discovered that the budget will allow five more samples to be tested. These tests are conducted and none of them fail. For now, would you say that this new chemical should pass?

## Q15.  Beta binomial
Let $𝑋∣\theta ∼Binomial(9,\theta)$  and assume a $Beta(a,b)$  prior for $\theta$. Suppose your prior guess (prior expectation) for  $\theta$ is 0.4 and you wish to use a prior effective sample size of 5, what values of $a$ and $b$ should you use?


## Q16.  Poisson for  cookies

We use a Poisson likelihood to model the number of chips per cookie, and a conjugate gamma prior on $\lambda$, the expected number of chips per cookie. Suppose your prior expectation for $\lambda$  is 8. The conjugate prior with mean 8 and effective sample size of 2 is $\text{Gamma}(a,2)$ ). Find the value of $a$.


## Q17.  Poisson for cookies

 The conjugate prior with mean 8 and standard deviation 1 is $\text{Gamma}(a,8)$. Find the value of $a$.

## Q18. Poisson for cookies

- Suppose you are not very confident in your prior guess of 8, so you want to use a prior effective sample size of 1/100 cookies. Then the conjugate prior is Gamma(𝑎,0.01). Find the value of 𝑎. Round your answer to two decimal places.

## Q19.  Poisson for cookies

Suppose you decide on the prior Gamma(8, 1), which has prior mean 8 and effective sample size of one cookie. We collect data, sampling five cookies and counting the chips in each. We find 9, 12, 10, 15, and 13 chips.  What is the posterior distribution for $\lambda$?


## Q20.  Poisson for cookies

Continuing the previous question, draw the prior density (dotted line) and posterior density (solid line) of $\lambda$? what is the posterior mean for $\lambda$?  Find the lower end of a 90% equal-tailed credible interval for $\lambda$


## Q21.  Poisson for cookies

Suppose that in addition to the five cookies reported, we observe an additional ten cookies with 109 total chips. What is the new posterior distribution for $\lambda$, the expected number of chips per cookie?


## Q22.  Poisson for calling rate

A retailer notices that a certain type of customer tends to call their customer service hotline more often than other customers, so they begin keeping track. They decide a Poisson process model is appropriate for counting calls, with calling rate $\theta$ calls per customer per day.

The model for the total number of calls is then
$$
Y \sim \text{Poisson}(n \cdot t \cdot \theta)
$$

where $n$ is the number of customers in the group and $t$ is the number of days. That is, if we observe the calls from a group with 24 customers for 5 days, the expected number of calls would be $24 \cdot 5 \cdot \theta = 120 \cdot \theta.$
The likelihood for $Y$ is then
$$
f(y \mid \theta) = \frac{(n t \theta)^y e^{-n t \theta}}{y!} \propto \theta^y e^{-n t \theta}.
$$
This model also has a conjugate gamma prior $\theta \sim \text{Gamma}(a, b)$
which has density:
$$
f(\theta) = \frac{b^a}{\Gamma(a)} \theta^{a-1} e^{-b \theta} \propto \theta^{a-1} e^{-b \theta}.
$$

Find the posterior distribution for $\theta$.

On average, the retailer receives 0.01 calls per customer per day. To give this group the benefit of the doubt, they set the prior mean for $\theta$ at 0.01 with standard deviation 0.5. Suppose there are $n=24$ customers in this particular group of interest, and the retailer monitors calls from these customers for $t=5$ days. They observe a total of $y=6$ calls from this group. Draw with the vertical dashed line shows the average calling rate of 0.01, add plot for prior and posterior distribution. Does this posterior inference for $\theta$ suggest that the group has a higher calling rate than the average of 0.01 calls per customer per day? 

