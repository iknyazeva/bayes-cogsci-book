# Big quize for continuous distribution

## Recap Bayes

1.  Which of the following is one major difference between the frequentist and Bayesian approach to modeling data?
    *   Frequentist models require a guess of parameter values to initialize models while Bayesian models require initial distributions for the parameters.
    *   Frequentist models are deterministic (don't use probability) while Bayesian models are stochastic (based on probability).
    *   Frequentists treat the unknown parameters as fixed (constant) while Bayesians treat unknown parameters as random variables.
    *   The frequentist paradigm treats the data as fixed while the Bayesian paradigm considers data to be random.


## Exponential likelihood with gamma prior


1. Recall that we used the conjugate Gamma prior for $\lambda$, which represents the arrival rate of buses per minute. Suppose our prior belief about this rate is that it has a mean of $\frac{1}{20}$ arrivals per minute and a standard deviation of $\frac{1}{5}$. Given this, the prior follows a $\text{Gamma}(a, b)$ distribution, where $a = \frac{1}{16}$. Determine the value of $b$
2.  Suppose that we wish to use a prior with the same mean (1/20), but with effective sample size of one arrival. Then the prior for $\lambda$  is $\text{Gamma}(1, 20)$ . In addition to the original $Y_1‚Äã=12$, we observe the waiting times for four additional busses: $ùëå_2=15, Y_3‚Äã=8, Y_4‚Äã=13.5, Y_5‚Äã=25$.
3. Use python to find the posterior probability that $\lambda <1/10$


## Earthquake Rate Modeling in California (2015) 

The United States Geological Survey maintains a list of significant earthquakes worldwide. In this analysis, we will model the rate of earthquakes with a magnitude of 4.0 or greater in the state of California during the year 2015. To understand the timing of these earthquakes, we will employ an independent and identically distributed (iid) exponential model on the waiting time between significant earthquakes. This approach is appropriate under the following assumptions: 
	1. Earthquake events are independent. 
	2. The rate at which earthquakes occur does not change during the year. 
	3. The earthquake hazard rate remains constant, meaning that the probability of an earthquake occurring tomorrow is the same regardless of whether the last earthquake was one day ago or 100 days ago.

Let $Y_i$ denote the waiting time in days between the ith earthquake and the following earthquake. Our model is defined as follows: $Y_i \sim \text{iid} \, \text{Exponential}(\lambda)$ In this model, the expected waiting time between earthquakes is represented as: $E(Y) = \frac{1}{\lambda} \, \text{days}$ For our analysis, we will assume a conjugate prior for $\lambda$, which follows a Gamma distribution: $\lambda \sim \text{Gamma}(a, b)$ We will set our prior expectation for $\lambda$ to be $\frac{1}{30}$, and we will utilize a prior effective sample size of one interval between earthquakes. This setting allows us to incorporate prior knowledge into our model effectively.

1. What is the value of ùëé?
2. What is the value of ùëè?
3.  If the significant earthquakes of magnitude 4.0+ in the state of California during 2015 occurred on the following dates: January 4, January 20, January 28, May 22, July 21, July 25, August 17, September 16, December 30. What will be the data vector?
4. What will the posterior distribution be? 
5. Use python to calculate the upper end of the 95% equal-tailed credible interval for $\lambda$, the rate of major earthquakes in events per day. 
6. Draw posterior predictive density for new value


## Normal data

1.  Suppose we have a statistical model with unknown parameter $\theta$, and we assume a normal prior $\theta \sim N(\mu_0, \sigma_0^2)$, where $\mu_0$ is the prior mean and $\sigma_0^2$ is the prior variance. What does increasing $\sigma_0^2$ say about our prior beliefs about $\theta$?
    *   Increasing the variance of the prior **narrows** the range of what we think $\theta$ might be, indicating **greater** confidence in our prior mean guess $\mu_0$.
    *   Increasing the variance of the prior **narrows** the range of what we think $\theta$ might be, indicating **less** confidence in our prior mean guess $\mu_0$.
    *   Increasing the variance of the prior **widens** the range of what we think $\theta$ might be, indicating **less** confidence in our prior mean guess $\mu_0$.
    *   Increasing the variance of the prior **widens** the range of what we think $\theta$ might be, indicating **greater** confidence in our prior mean guess $\mu_0$.

2. Suppose you are trying to calibrate a thermometer by testing the temperature it reads when water begins to boil. Because of natural variation, you take $ùëõ$ independent measurements (experiments) to estimate $\theta$, the mean temperature reading for this thermometer at the boiling point. Assume a normal likelihood for these data, with mean $\theta$ and known variance $\sigma^2=0.25$  (which corresponds to a standard deviation of 0.5 degrees Celsius). Suppose your prior for $\theta$ is (conveniently) the conjugate normal. You know that at sea level, water should boil at 100 degrees Celsius, so you set the prior mean at $m_0‚Äã=100$.

	1. If you specify a prior variance $s_0^2‚Äã$  for $\theta$, write the model for your measurements $Y_i‚Äã, i=1,‚Ä¶,n$ ?
	2. You decide you want the prior to be equivalent (in effective sample size) to one measurement. What value should you select for $s_0^2$‚Äã the prior variance of $\theta$ ? Round your answer to two decimal places.



3. Thermometer calibration. 
	You collect the following $n=5$ measurements: (94.6, 95.4, 96.2, 94.9, 95.9).
	What is the posterior distribution for $\theta$?


	1. Use python to find the upper end of a 95% equal-tailed credible interval for $\theta$.
	2. After collecting these data, is it reasonable to conclude that the thermometer is biased toward low values?
	3. What is the posterior predictive distribution of a single future observation $ùëå$

4. Restaurantes

	Your friend moves from city A to city B and is delighted to find her favorite restaurant chain at her new location. After several meals, however, she suspects that the restaurant in city B is less generous. She decides to investigate.

	She orders the main dish on 30 randomly selected days throughout the year and records each meal's weight in grams. You still live in city A, so you assist by performing the same experiment at your restaurant. Assume that the dishes are served on identical plates (measurements subtract the plate's weight), and that your scale and your friend‚Äôs scale are consistent.

	Your friend investigates the three observations above 700 grams and discovers that she had ordered the incorrect meal on those dates. She removes these observations from the data set and proceeds with the analysis using $n = 27$.

	She assumes a normal likelihood for the data with unknown mean $\mu$ and unknown variance $\sigma^2$. She uses the model where, conditional on $\sigma^2$, the prior for $\mu$ is normal with mean $m$ and variance $\sigma^2 / w$. Next, the marginal prior for $\sigma^2$ is $\text{Inverse-Gamma}(a, b)$.

	Your friend's prior guess on the mean dish weight is 500 grams, so we set $m = 500$. She is not very confident with this guess, so we set the prior effective sample size $w = 0.1$. Finally, she sets $a = 3$ and $b = 200$.


	 Simulate a large number of draws (at least 300) from the prior for $\sigma^2$ and report your approximate prior mean from these draws.


	With the $n = 27$ data points, your friend calculates the sample mean $\bar{y} = 609.7$ and the sample variance 

	$$
	s^2 = \frac{1}{n - 1} \sum (y_i - \bar{y})^2 = 401.8
	$$

	Using the update formulas for normal distribution calculate the posterior distributions for parameters. 

	$$
	\sigma^2 \mid y \sim \text{Inverse-Gamma}(a', b')
	$$

	$$
	\mu \mid \sigma^2, y \sim N\left(m', \frac{\sigma^2}{w + n}\right)
	$$

	Perform the posterior simulation and compute your approximate 95% equal-tailed credible interval for $\mu$.


	You complete your experiment at Restaurant A with $n = 30$ data points, which appear to be normally distributed. You calculate the sample mean $\bar{y} = 622.8$ and sample variance 

	$$
	s^2 = \frac{1}{n - 1} \sum (y_i - \bar{y})^2 = 403.1
	$$



	Treating the data from Restaurant A as independent from Restaurant B, we can now attempt to answer your friend's original question: is Restaurant A more generous? To do so, we can compute posterior probabilities of hypotheses like $\mu_A > \mu_B$. 

	This is a simple task if we have simulated draws for $\mu_A$ and $\mu_B$. For $i = 1, \dots, N$ (the number of simulations drawn for each parameter), make the comparison $\mu_A > \mu_B$ using the $i$-th draw for $\mu_A$ and $\mu_B$. Then, count how many of these return a TRUE value and divide by $N$, the total number of simulations.



## Priors recap


1. Suppose we flip a coin five times to estimate $\theta$, the probability of obtaining heads. We use a Bernoulli likelihood for the data and a non-informative (and improper) Beta(0,0) prior for $\theta$. We observe the following sequence: (H, H, H, T, H). Because we observed at least one H and at least one T, the posterior is proper. What is the posterior distribution for $\theta$?

2. Continuing the previous question, what is the posterior mean for $\theta$? Round your answer to one decimal place.

3. Consider again the thermometer calibration problem .
Assume a normal likelihood with unknown mean $\theta$  and known variance $\sigma^2=0.25$. Now use the non-informative (and improper) flat prior for $\theta$ across all real numbers. This is equivalent to a conjugate normal prior with variance equal to $\infty$.

- You collect the following  $n=5$ measurements: $(94.6, 95.4, 96.2, 94.9, 95.9)$. What is the posterior distribution for $\theta$?

3.  Scientist A studies the probability of a certain outcome of an experiment and calls it $theta$. To be non-informative, he assumes a $\text{Uniform}(0,1)$ prior for $\theta$. Scientist B studies the same outcome of the same experiment using the same data, but wishes to model the odds 

	$$
	\phi = \frac{\theta}{1 - \theta}
	$$

	Scientist B places a uniform distribution on $\phi$. If she reports her inferences in terms of the probability $\theta$, will they be equivalent to the inferences made by Scientist A?


